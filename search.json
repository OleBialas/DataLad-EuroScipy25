[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "DataLad EuroScipy 2025",
    "section": "",
    "text": "Welcome to the website for the DataLad1 tutorial at EuroScipy 2025! Here, you can access all exercise materials and slides. The rest of this page walks you through the setup process and resources for the tutorial."
  },
  {
    "objectID": "index.html#installation",
    "href": "index.html#installation",
    "title": "DataLad EuroScipy 2025",
    "section": "Installation",
    "text": "Installation\nDataLad requires Git and Python 3.8 or later. All other dependencies can be installed via pip. If you want to learn more about DataLad and its dependencies, check out the handbook.\n\nGit\nTo install and configure git follow these steps:\n\nInstall Git: https://git-scm.com/downloads\nConfigure your git user name: git config --global user.name \"user\"\nConfigure your git user email: git config --global user.email \"user@mail.com\"\n\n\n\nPython\nWe recommend to create a new virtual environment for this project with Python 3.8 or later using a tool like uv, pixi or conda. In your dedicated environment, you can install DataLad and its dependencies via pip: pip install datalad datalad-next git-annex 2. Some of the exercises require you to run Python scripts — you’ll have to install their dependencies as well: pip install pandas seaborn.\n\n\nOther\nWe also recommend to set the following git config, which enables the full functionality of the DataLad-next extension (installed above) by allowing it to override the behavior of the core DataLad package.\n\ngit config --global --add datalad.extensions.load next"
  },
  {
    "objectID": "index.html#data",
    "href": "index.html#data",
    "title": "DataLad EuroScipy 2025",
    "section": "Data",
    "text": "Data\nTo demonstrate DataLad’s data management capabilities, we’ll use a dataset hosted on GIN. You don’t have to download it upfront since cloning the dataset is part of the exercises. The data contains measurements from different penguin species and was originally published by Kristen B Gorman and colleagues 3."
  },
  {
    "objectID": "index.html#further-reading",
    "href": "index.html#further-reading",
    "title": "DataLad EuroScipy 2025",
    "section": "Further Reading",
    "text": "Further Reading\nIf you want to learn more about DataLad, you can check out the handbook which contains lots of beginner-friendly and advanced tutorials as well as the technical documentation which contains detailed descriptions of all DataLad features. If you want to learn more about the underlying file system operations, the git-annex documentation is a useful resource as well."
  },
  {
    "objectID": "index.html#footnotes",
    "href": "index.html#footnotes",
    "title": "DataLad EuroScipy 2025",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nHalchenko et al., (2021). DataLad: distributed system for joint management of code, data, and their relationship. Journal of Open Source Software, 6(63), 3262, https://doi.org/10.21105/joss.03262.↩︎\nGit-annex is not a Python package – it’s written in Haskell, and plenty of installation methods are available – but it is also distributed as a Python wheels package, making for a very convenient way to install as a dependency of DataLad in a virtual environment.↩︎\nGorman, K. B., Williams, T. D., & Fraser, W. R. (2014). Ecological sexual dimorphism and environmental variability within a community of Antarctic penguins (genus Pygoscelis): https://doi.org/10.1371/journal.pone.0090081.↩︎"
  },
  {
    "objectID": "about/michal.html",
    "href": "about/michal.html",
    "title": "Michał Szczepanik",
    "section": "",
    "text": "Michał graduated with BSc and MSc in Neuroinformatics, an interdisciplinary program at the Faculty of Physics, University of Warsaw. This is where he started using Python, as a first-year-student, back in 2011.\nHe completed his PhD at the Laboratory of Brain Imaging (LOBI), Nencki Institute. The PhD was part of the translational project Neural correlates of emotional contagion in humans, which applied fMRI and other methods to study observational fear conditioning. During the time, Michał became particularly interested in the data management side of neuroscience, as LOBI served as a core MRI facility.\nHis current work at the Psychoinformatics Group, INM-7, Forschungszentrum Jülich is in the area of research data management and research software development. It revolves around the data management software DataLad. His responsibilities include bug fixing, user support, technical writing, teaching – and sometmes just trying out new things.\n\nmszczepanik.eu/\n@doktorpanik@masto.ai\norcid.org/0000-0002-4028-2087"
  },
  {
    "objectID": "slides/review3.html#bare-git-repo-as-push-target",
    "href": "slides/review3.html#bare-git-repo-as-push-target",
    "title": "Exercise Review 3",
    "section": "Bare Git repo as push target",
    "text": "Bare Git repo as push target\n\nbare: no worktree; contents of .git directly in the directory\ncan be pushed to and cloned from\nwhen shared, pushes won’t break worktree (b/c there is none)\ncan be put on a server 1\nwith git-annex on the machine, can store git+annex\ngood model for Git & git-annex aware services\n\nhttps://git-scm.com/book/en/v2/Git-on-the-Server-Getting-Git-on-a-Server"
  },
  {
    "objectID": "slides/review3.html#git-remotes-vs-git-annex-special-remotes",
    "href": "slides/review3.html#git-remotes-vs-git-annex-special-remotes",
    "title": "Exercise Review 3",
    "section": "Git remotes vs git-annex special remotes",
    "text": "Git remotes vs git-annex special remotes\n\nstore and retrieve file content (not Git repo)1\ngit annex initremote ...\nbuilt-in and external implementations\n\nhttps://git-annex.branchable.com/special_remotes/\n\n\nbut actually also Git repo bundles, with git-remote-annex"
  },
  {
    "objectID": "slides/review3.html#what-contents-are-actually-being-pushed",
    "href": "slides/review3.html#what-contents-are-actually-being-pushed",
    "title": "Exercise Review 3",
    "section": "What contents are actually being pushed",
    "text": "What contents are actually being pushed\n\ndatalad push acts on the contents you already got\n\nie. keys you have locally\n\ngit annex copy [--from=remote|--to=remote]\n\ndownload and upload (no magic)"
  },
  {
    "objectID": "slides/review3.html#file-content-can-be-distributed-across-multiple-sources",
    "href": "slides/review3.html#file-content-can-be-distributed-across-multiple-sources",
    "title": "Exercise Review 3",
    "section": "File content can be distributed across multiple sources",
    "text": "File content can be distributed across multiple sources\n\nrecall git annex whereis (n copies)\nkeyword: distributed data management\n\ncan have multiple special remotes\n\ngit annex configuration:\n\npreferred content\nnumcopies"
  },
  {
    "objectID": "slides/review3.html#rd-party-services",
    "href": "slides/review3.html#rd-party-services",
    "title": "Exercise Review 3",
    "section": "3rd party services",
    "text": "3rd party services"
  },
  {
    "objectID": "notebooks/exercises2.html",
    "href": "notebooks/exercises2.html",
    "title": "Part 2: Tracking Changes in DataLad Datasets",
    "section": "",
    "text": "DataLad keeps track of all changes made to your dataset. In this section, you will add new content to the penguins dataset and see how these changes are tracked in the git log of your repository.\n\n\n\n\n\n\n\nCommand\nDescription\n\n\n\n\ndatalad status\nShow any untracked changes in the current dataset\n\n\ndatalad save\nSave any untracked changes in the current dataset\n\n\ndatalad save -m \"hi\"\nSave untracked changes and add the message \"hi\"\n\n\ndatalad unlock file.txt\nUnlock file.txt to make it modifiable\n\n\ngit log\nView the dataset’s history, stored in the git log\n\n\ngit log -3\nView the last 3 entries in the git log\n\n\n\n\nExercise 1 Create a new file in the penguins folder called penguin_species.txt and add the species names gentoo and adelie. Then, save the file and run datalad status to see the untracked changes.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nOn Linux/macOS:\necho -e \"gentoo\\nadelie\" &gt; penguin_species.txt\ndatalad status\nOn Windows:\n(echo gentoo & echo adelie) &gt; penguin_species.txt\nor just use a text editor of your choice.\n\n\n\n\nExercise 2 Use datalad save to save the untracked changes with the message \"list penguin species\".\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\ndatalad save -m \"list penguin species\"\n\n\n\n\nExercise 3 Open the git log and find the entry created by the previous datalad save command\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nOn Linux/macOS this should be the last (i.e. top) entry, on Windows it will be the second-to-last.\ngit log\n\n\n\n\nExercise 4 Use datalad unlock to unlock the penguin_species.txt file and append chinstrap to the list. Then, run datalad save again with a message to save the changes\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nOn Linux/macOS:\ndatalad unlock penguin_species.txt\necho -e \"chinstrap\" &gt;&gt; penguin_species.txt\ndatalad save -m \"add chinstrap\"\nOn Windows:\ndatalad unlock penguin_species.txt\necho chinstrap &gt;&gt; penguin_species.txt\ndatalad save -m \"add chinstrap\"\n\n\n\n\nExercise 5 Open the git log again and find the entry from the datalad save command above.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\ngit log"
  },
  {
    "objectID": "notebooks/exercises2.html#modifying-a-dataset",
    "href": "notebooks/exercises2.html#modifying-a-dataset",
    "title": "Part 2: Tracking Changes in DataLad Datasets",
    "section": "",
    "text": "DataLad keeps track of all changes made to your dataset. In this section, you will add new content to the penguins dataset and see how these changes are tracked in the git log of your repository.\n\n\n\n\n\n\n\nCommand\nDescription\n\n\n\n\ndatalad status\nShow any untracked changes in the current dataset\n\n\ndatalad save\nSave any untracked changes in the current dataset\n\n\ndatalad save -m \"hi\"\nSave untracked changes and add the message \"hi\"\n\n\ndatalad unlock file.txt\nUnlock file.txt to make it modifiable\n\n\ngit log\nView the dataset’s history, stored in the git log\n\n\ngit log -3\nView the last 3 entries in the git log\n\n\n\n\nExercise 1 Create a new file in the penguins folder called penguin_species.txt and add the species names gentoo and adelie. Then, save the file and run datalad status to see the untracked changes.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nOn Linux/macOS:\necho -e \"gentoo\\nadelie\" &gt; penguin_species.txt\ndatalad status\nOn Windows:\n(echo gentoo & echo adelie) &gt; penguin_species.txt\nor just use a text editor of your choice.\n\n\n\n\nExercise 2 Use datalad save to save the untracked changes with the message \"list penguin species\".\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\ndatalad save -m \"list penguin species\"\n\n\n\n\nExercise 3 Open the git log and find the entry created by the previous datalad save command\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nOn Linux/macOS this should be the last (i.e. top) entry, on Windows it will be the second-to-last.\ngit log\n\n\n\n\nExercise 4 Use datalad unlock to unlock the penguin_species.txt file and append chinstrap to the list. Then, run datalad save again with a message to save the changes\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nOn Linux/macOS:\ndatalad unlock penguin_species.txt\necho -e \"chinstrap\" &gt;&gt; penguin_species.txt\ndatalad save -m \"add chinstrap\"\nOn Windows:\ndatalad unlock penguin_species.txt\necho chinstrap &gt;&gt; penguin_species.txt\ndatalad save -m \"add chinstrap\"\n\n\n\n\nExercise 5 Open the git log again and find the entry from the datalad save command above.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\ngit log"
  },
  {
    "objectID": "notebooks/exercises2.html#running-scripts-with-datalad",
    "href": "notebooks/exercises2.html#running-scripts-with-datalad",
    "title": "Part 2: Tracking Changes in DataLad Datasets",
    "section": "Running Scripts with DataLad",
    "text": "Running Scripts with DataLad\nOften, we won’t edit our dataset manually but run scripts that do so. In this section you will use DataLad to run Python scripts and track the changes made by them. You are also going to use the dataset’s history to re-run the commands.\n\n\n\n\n\n\n\nCommand\nDescription\n\n\n\n\ndatalad run \"python script.py\"\nRun the python script script.py\n\n\ndatalad run --input \"data.csv\" --output \"figure.png\" \"python script.py\"\nRun script.py with input \"data.csv\" and output \"figure.png\"\n\n\ngit log\nView the dataset’s history stored in the git log\n\n\ndatalad rerun a268d8ca22b6\nRerun the command from the git log with the checksum starting with a268d8ca22b6e87959\n\n\ndatalad rerun --since a268d8ca22b6\nRerun ALL commands --since the one with the checksum starting with a268d8ca22b6e87959\n\n\n\n\nExercise 6 Try to run the python script in code/aggregate_culmen_data.py. What error message do you observe?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\ndatalad run \"python code/aggregate_culmen_data.py\"\nCurrently, the dataset does not contain the annexed content for the required files. On Linux/macOS, this will result in a FileNotFoundError: [Errno 2] No such file or directory On Windows, you’ll see KeyError: \"None of [Index(['Culmen Length (mm)', 'Culmen Depth (mm)', 'Species'], dtype='object')] are in the [columns]\" because Python will actually open the pointer file and crash because it can’t find the required data.\n\n\n\n\nExercise 7 Run the same script with the data/ folder as --input and the file \"results/penguin_culmens.csv\" as --output.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\ndatalad run --input \"data/\" --output \"results/penguin_culmens.csv\" \"python code/aggregate_culmen_data.py\"\n\n\n\n\nExercise 8 Open the git log to view the entry created by the datalad run command. Then, copy the checksum of that commit\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\ngit log\nThe git log entry should look like this:\ncommit af78031c9ca45d3c349a692b0afd332178639e64 (master)\nAuthor: Ole Bialas &lt;ole.bialas@posteo.de&gt;\nDate:   Thu Jul 31 11:14:00 2025 +0200\n\n    [DATALAD RUNCMD] python code/aggregate_culmen_data.py\n\n    === Do not change lines below ===\n    {\n     \"chain\": [],\n     \"cmd\": \"python code/aggregate_culmen_data.py\",\n     \"dsid\": \"3a8aacc5-85f0-4114-adee-fcfa7d21a5df\",\n     \"exit\": 0,\n     \"extra_inputs\": [],\n     \"inputs\": [\n      \"data\"\n     ],\n     \"outputs\": [\n      \"results/penguin_culmens.csv\"\n     ],\n     \"pwd\": \".\"\n    }\n    ^^^ Do not change lines above ^^^\nThe checksum is displayed on the first line, after “commit”: af78031c9ca45d3c349a692b0afd332178639e64\n\n\n\n\nExercise 9 Use the copied checksum to rerun the previous datalad run command\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe command looks like this but the checksum will be different for everyone:\ndatalad rerun af78031c9ca45d3c349a692b0afd332178639e64\n\n\n\n\nExercise 10 Run the script code/plot_culmen_length_vs_depth.py — it takes results/penguin_culmens.csv as --input and produces results/culmen_length_vs_depth.png as an output.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\ndatalad run --input \"results/penguin_culmens.csv\" --output \"results/culmen_length_vs_depth.png\" \"python code/plot_culmen_length_vs_depth.py\"\n\n\n\n\nExercise 11 Use the checksum of the very first commit (that says [DATALAD] new dataset) to re-run everything --since this commit (i.e. the whole analysis).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\ndatalad rerun --since 4c2b9dcd5c745b519095f7bf6612bbe20f7ae9bb"
  },
  {
    "objectID": "notebooks/exercises2.html#further-reading",
    "href": "notebooks/exercises2.html#further-reading",
    "title": "Part 2: Tracking Changes in DataLad Datasets",
    "section": "Further reading",
    "text": "Further reading\n\nFor more on DataLad run and the comparison of how Git and git-annex handle files, see these chapters of the DataLad Handbook:\n\nDataLad, run!\nUnder the hood: git-annex\n\nFor even more on git-annex under the hood, see git-annex documentation:\n\nUnlocked files\nPointer files\nAdjusted branches"
  },
  {
    "objectID": "notebooks/exercises1.html",
    "href": "notebooks/exercises1.html",
    "title": "Part 1: Working with DataLad Datasets",
    "section": "",
    "text": "In this section, you are going to clone an existing DataLad dataset and download its contents. While the datalad API is universal, the commands for navigating the data set differ between operating systems (see table below).\n\nTerminal commands\n\n\n\n\n\n\n\nLinux/macOS\nWindows\nDescription\n\n\n\n\nls -a\ndir /a\nList the content of the current directory (including hidden files)\n\n\nls -a data\ndir /a data\nList the content of the data directory\n\n\ndu -sh\ndir /s\nGet the disk usage of the current directory\n\n\ndu -sh data\ndir /s data\nGet the disk usage of the data directory\n\n\ncd data\ncd data\nChange the directory to data\n\n\n\n\nDataLad commands\n\n\n\n\n\n\nCommand\nDescription\n\n\n\n\ndatalad clone https://example.com\nClone the data set from example.com\n\n\ndatalad get folder/\nGet the file content of the folder/\n\n\ndatalad get folder/image.png\nGet the file content of the file image.png\n\n\ndatalad drop folder/\nDrop the file content of the folder/\n\n\n\nOpen a terminal to do the following exercises. For Windows users, we recommend CMD (the solutions will assume you are using CMD).\n\nExercise 1 Clone the dataset from https://gin.g-node.org/obi/penguins\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\ndatalad clone https://gin.g-node.org/obi/penguins\n\n\n\n\nExercise 2 Change the directory to penguins and list the directory’s content\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nOn Linux/macOS:\ncd penguins\nls -a\nOn Windows:\ncd penguins\ndir /a\n\n\n\n\nExercise 3 Check the disk usage of the penguins directory\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nOn Linux/macOS:\ndu -sh\nOn Windows:\ndir /s\n\n\n\n\nExercise 4 Get the content of the examples subdirectory\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\ndatalad get examples\n\n\n\n\nExercise 5 Check the disk usage of the penguins directory again\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nOn Linux/macOS:\ndu -sh\nOn Windows:\ndir /s\n\n\n\n\nExercise 6 Drop the content of examples/chinstrap.jpg and check the disk usage again\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\ndatalad drop examples/chinstrap.jpg\nOn Linux/macOS:\ndu -sh\nOn Windows:\ndir /s"
  },
  {
    "objectID": "notebooks/exercises1.html#consuming-existing-datasets",
    "href": "notebooks/exercises1.html#consuming-existing-datasets",
    "title": "Part 1: Working with DataLad Datasets",
    "section": "",
    "text": "In this section, you are going to clone an existing DataLad dataset and download its contents. While the datalad API is universal, the commands for navigating the data set differ between operating systems (see table below).\n\nTerminal commands\n\n\n\n\n\n\n\nLinux/macOS\nWindows\nDescription\n\n\n\n\nls -a\ndir /a\nList the content of the current directory (including hidden files)\n\n\nls -a data\ndir /a data\nList the content of the data directory\n\n\ndu -sh\ndir /s\nGet the disk usage of the current directory\n\n\ndu -sh data\ndir /s data\nGet the disk usage of the data directory\n\n\ncd data\ncd data\nChange the directory to data\n\n\n\n\nDataLad commands\n\n\n\n\n\n\nCommand\nDescription\n\n\n\n\ndatalad clone https://example.com\nClone the data set from example.com\n\n\ndatalad get folder/\nGet the file content of the folder/\n\n\ndatalad get folder/image.png\nGet the file content of the file image.png\n\n\ndatalad drop folder/\nDrop the file content of the folder/\n\n\n\nOpen a terminal to do the following exercises. For Windows users, we recommend CMD (the solutions will assume you are using CMD).\n\nExercise 1 Clone the dataset from https://gin.g-node.org/obi/penguins\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\ndatalad clone https://gin.g-node.org/obi/penguins\n\n\n\n\nExercise 2 Change the directory to penguins and list the directory’s content\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nOn Linux/macOS:\ncd penguins\nls -a\nOn Windows:\ncd penguins\ndir /a\n\n\n\n\nExercise 3 Check the disk usage of the penguins directory\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nOn Linux/macOS:\ndu -sh\nOn Windows:\ndir /s\n\n\n\n\nExercise 4 Get the content of the examples subdirectory\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\ndatalad get examples\n\n\n\n\nExercise 5 Check the disk usage of the penguins directory again\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nOn Linux/macOS:\ndu -sh\nOn Windows:\ndir /s\n\n\n\n\nExercise 6 Drop the content of examples/chinstrap.jpg and check the disk usage again\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\ndatalad drop examples/chinstrap.jpg\nOn Linux/macOS:\ndu -sh\nOn Windows:\ndir /s"
  },
  {
    "objectID": "notebooks/exercises1.html#checking-file-identity-and-location-with-git-annex",
    "href": "notebooks/exercises1.html#checking-file-identity-and-location-with-git-annex",
    "title": "Part 1: Working with DataLad Datasets",
    "section": "Checking File Identity and Location with git-annex",
    "text": "Checking File Identity and Location with git-annex\nSince DataLad is built on top of git-annex, you can use its commands on any DataLad dataset. In this section, you’ll use git-annex to get information on the dataset and locate its file contents.\n\n\n\n\n\n\n\nCommand\nDescription\n\n\n\n\ngit annex info\nShow the git-annex information for the whole dataset\n\n\ngit annex info folder/image.png\nShow the git-annex information for the file image.png\n\n\ngit annex whereis folder/image.png\nList the repositories that have the file content for image.png\n\n\n\n\nExercise 7 Display the git annex info for the file examples/gentoo.jpg. What is the size of that file? Is it present on your machine?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\ngit annex info examples/gentoo.jpg\nThe file is 4.81 megabtyes and it should be present since we previously loaded the content of the examples folder.\n\n\n\n\nExercise 8 Display the git-annex info of the whole data set. How many annexed files are there in the working tree?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\ngit annex info\nThe number of annexed files is displayed in this line: annexed files in working tree: 6\n\n\n\n\nExercise 9 Use git annex whereis to list the repositories that have the file content for the image examples/gentoo.jpg.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\ngit annex whereis examples/gentoo.jpg\n\n\n\n\nExercise 10 Use git annex whereis to list the repositories that have the file content for the table data/table_220.csv. How does this differ from the list of repositories that contain the content for gentoo.jpg?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\ngit annex whereis data/table_220.csv\nThe table is not stored in the local repository, listed in the line marked [here]."
  },
  {
    "objectID": "notebooks/exercises3.html",
    "href": "notebooks/exercises3.html",
    "title": "Part 3: Creating Backups and Sharing DataLad Datasets",
    "section": "",
    "text": "Command\nDescription\n\n\n\n\ngit init --bare ~/mydir\nCreate a --bare repository called mydir in the home directory (on Linux/macOS)\n\n\ngit init --bare %USERPROFILE%\\mydir\nCreate a --bare repository called mydir in the home directory (on Windows / CMD)\n\n\ngit init --bare \"$env:USERPROFILE\\mydir\"\nCreate a --bare repository called mydir in the home directory (on Windows / PowerShell)\n\n\ndatalad siblings\nList all siblings of the current dataset\n\n\ndatalad sibings add --name new --url ~/mydir\nAdd the repository at ~/mydir as a new sibling with the name new\n\n\ndatalad push --to new\nPush the dataset content to the sibling named new\n\n\n\n\nExercise 1 List all siblings of the current dataset.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\ndatalad siblings\n\n\n\n\nExercise 2 Initialize a --bare git repository at a path outside of this dataset.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nOn Linux/macOS\ngit init --bare ~/penguins_backup\nOn Windows\ngit init --bare %USERPROFILE%\\penguins_backup\n\n\n\n\nExercise 3 add a new sibling to the dataset using the path to the newly created git repository as the --url. Then, list all siblings to confirm it was added.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nOn Linux/macOS\ndatalad siblings add --name backup --url ~/penguins_backup\ndatalad siblings\nOn Windows\ndatalad siblings add --name backup --url %USERPROFILE%\\penguins_backup\ndatalad siblings\n\n\n\n\nExercise 4 Push the dataset to the new sibling twice.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nWe need to push tiwce because the first push initializes the repository’s annex ID and the second (and each subsequent) push actually tranfer the annexed files.\ndatalad push --to backup\ndatalad push --to backup\n\n\n\n\nExercise 5 Move to a directory outside of this dataset and clone the new sibling dataset.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nOn Linux/macOS\ncd ..\ndatalad clone ~/penguins_backup\nOn Windows\ndatalad clone %USERPROFILE%\\penguins_backup"
  },
  {
    "objectID": "notebooks/exercises3.html#creating-a-backup",
    "href": "notebooks/exercises3.html#creating-a-backup",
    "title": "Part 3: Creating Backups and Sharing DataLad Datasets",
    "section": "",
    "text": "Command\nDescription\n\n\n\n\ngit init --bare ~/mydir\nCreate a --bare repository called mydir in the home directory (on Linux/macOS)\n\n\ngit init --bare %USERPROFILE%\\mydir\nCreate a --bare repository called mydir in the home directory (on Windows / CMD)\n\n\ngit init --bare \"$env:USERPROFILE\\mydir\"\nCreate a --bare repository called mydir in the home directory (on Windows / PowerShell)\n\n\ndatalad siblings\nList all siblings of the current dataset\n\n\ndatalad sibings add --name new --url ~/mydir\nAdd the repository at ~/mydir as a new sibling with the name new\n\n\ndatalad push --to new\nPush the dataset content to the sibling named new\n\n\n\n\nExercise 1 List all siblings of the current dataset.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\ndatalad siblings\n\n\n\n\nExercise 2 Initialize a --bare git repository at a path outside of this dataset.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nOn Linux/macOS\ngit init --bare ~/penguins_backup\nOn Windows\ngit init --bare %USERPROFILE%\\penguins_backup\n\n\n\n\nExercise 3 add a new sibling to the dataset using the path to the newly created git repository as the --url. Then, list all siblings to confirm it was added.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nOn Linux/macOS\ndatalad siblings add --name backup --url ~/penguins_backup\ndatalad siblings\nOn Windows\ndatalad siblings add --name backup --url %USERPROFILE%\\penguins_backup\ndatalad siblings\n\n\n\n\nExercise 4 Push the dataset to the new sibling twice.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nWe need to push tiwce because the first push initializes the repository’s annex ID and the second (and each subsequent) push actually tranfer the annexed files.\ndatalad push --to backup\ndatalad push --to backup\n\n\n\n\nExercise 5 Move to a directory outside of this dataset and clone the new sibling dataset.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nOn Linux/macOS\ncd ..\ndatalad clone ~/penguins_backup\nOn Windows\ndatalad clone %USERPROFILE%\\penguins_backup"
  },
  {
    "objectID": "notebooks/exercises3.html#bonus-sharing-your-dataset-online",
    "href": "notebooks/exercises3.html#bonus-sharing-your-dataset-online",
    "title": "Part 3: Creating Backups and Sharing DataLad Datasets",
    "section": "BONUS: Sharing your Dataset online",
    "text": "BONUS: Sharing your Dataset online\n\n\n\n\n\n\n\nCommand\nDescription\n\n\n\n\nssh-keygen\nGenerate a public and private authentication key pair\n\n\ndatalad siblings\nList all siblings of the current dataset\n\n\ndatalad sibings add --name gin --url git@gin.g-node.org:/user/repo.git\nAdd the gin repository at /https://gin.g-node.org/user/repo as a new sibling with the name gin\n\n\ndatalad push --to gin\nPush the dataset content to the sibling named gin\n\n\n\n\nExercise 6 Use ssh-keygen to generate a public and private key pair (you don’t have to use a passphrase). Note the location where the public key is stored, e.g. .ssh/id_ed25519.pub. Open the .pub file and copy the whole content — it should look something like this: ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIBOYcoRKZZLWA4FWECpW2K/fTOvuRYXBnBA6gcea2bFq &lt;user&gt;@&lt;computer&gt;\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nssh-keygen\n\n\n\n\nExercise 7 Login in to your GIN account, go to your user settings and add the copied ssh key. Now datalad should be able to connect to your GIN account! \n\n\nExercise 8 Create a new repository on GIN, make sure to NOT initialize it with a README. \n\n\nExercise 9 add a new sibling to the dataset using the --url of the newly created gin repository and confirm the connection. Then, list all siblings to confirm it was added.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nFor the repository in the image above, the command would look like this:\ndatalad siblings add --name gin --url git@gin.g-node.org:/adswa/DataLad-101.git\n\n\n\n\nExercise 10 Push the dataset to the new GIN sibling. Then, open the repository in your browser to confirm the content was pushed.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\ndatalad push --to gin\n\n\n\n\nExercise 11 Move to a directory outside of this dataset and clone the new GIN sibling.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nFor the repository in the image above, the command would look like this:\ncd ..\ndatalad clone datalad clone https://gin.g-node.org/adswa/DataLad-101"
  },
  {
    "objectID": "notebooks/exercises3.html#further-reading",
    "href": "notebooks/exercises3.html#further-reading",
    "title": "Part 3: Creating Backups and Sharing DataLad Datasets",
    "section": "Further reading",
    "text": "Further reading\nIn the examples above, the annex was published together with the Git repository. However, this is a bit of a special case, and in many scenarios they can be moved separately. For an overview and examples of several different publishing scenarios, see the Beyond shared infrastructure chapter of the DataLad handbook.\nGit-annex supports multiple options for publishing file contents; see the list of built-in special remotes. And for a very special case, in which the Git repository is placed by git-annex in a non-git-aware hosting, see git-remote annex.\nFinally, Forgejo is gaining popularity as a self-hosted software forge. Forgejo-aneksajo is a soft fork of Forgejo which adds git-annex capability. See also Collaborative infrastructure for a lab: Forgejo on the DataLad blog .s"
  },
  {
    "objectID": "slides/review2.html#git-versus-git-annex",
    "href": "slides/review2.html#git-versus-git-annex",
    "title": "Exercise Review 2",
    "section": "Git versus Git-annex",
    "text": "Git versus Git-annex\n\nData in datasets is either stored in Git or git-annex\nMatter of configuration; by default, everything is annexed"
  },
  {
    "objectID": "slides/review2.html#git-versus-git-annex-1",
    "href": "slides/review2.html#git-versus-git-annex-1",
    "title": "Exercise Review 2",
    "section": "Git versus Git-annex",
    "text": "Git versus Git-annex\nGit and git-annex handle files differently:\n\nFiles in Git are downloaded during clone, annexed contents are retrieved on demand (get).\nContent identity and availability information of annexed files are available after cloning.\nOn datalad save, annexed contents are hashed, moved to .git/annex/objects and symlinked.\nGit versions the symlink (content-identity), not the content.\nContent is “locked” (write-protected) against accidental modifications.\nFiles stored in Git are modifiable, annexed files are write-protected."
  },
  {
    "objectID": "slides/review2.html#annexed-files-on-windows",
    "href": "slides/review2.html#annexed-files-on-windows",
    "title": "Exercise Review 2",
    "section": "Annexed files on Windows",
    "text": "Annexed files on Windows\nWindows’ file system does not support symlinks1. Under these conditions, git-annex automatically operates in “adjusted unlocked mode”:\n\nFile contents are duplicated: One copy to edit, one to keep safe.\nNo (un)locking – trade-off: disk space use vs easy modification.\nAnnexed files are pointer files instead of symlinks: A text file with the contents “/annex/objects/” followed by the key.\nGit annex uses adjusted branches, which unlock on top the locked counterpart branch. Compatibility with other systems but confusing Git log and issues in some nesting usecases.\n\n\n\nOversimplification. It kind-of does, but not by default."
  },
  {
    "objectID": "slides/intro.html#a-community-project",
    "href": "slides/intro.html#a-community-project",
    "title": "Introduction to DataLad",
    "section": "A community project",
    "text": "A community project\n\n10+ years of ongoing development & maintenance 1\n100+ contributors across core, extensions, and Handbook\nstarted by:\n\nMichael Hanke (now: Psychoinformatics Lab, Forschungszentrum Jülich)\nYaroslav Halchenko (now: Center for Open Neuroscience, Dartmouth College)\n\nowes a lot to git-annex by Joey Hess & contributors\n\n\n\nThere is a website for this tutorial which you can find via this URL or the QR code you can see on this slide\nOn the landing page, you’ll find the installation instuctions so if you haven’t already please follow them to set up your environment\nYou’ll also find all exercises and slides for the tutorial. For example, to access this presentation go to slides &gt; introduction\n\n\nhttps://doi.org/10.34734/FZJ-2025-01847"
  },
  {
    "objectID": "slides/intro.html#a-piece-of-software",
    "href": "slides/intro.html#a-piece-of-software",
    "title": "Introduction to DataLad",
    "section": "A piece of software",
    "text": "A piece of software\n\nSoftware for data management\nWritten in Python\nBased on git and git-annex\nFOSS (MIT license)"
  },
  {
    "objectID": "slides/intro.html#exhaustive-tracking-of-research-components",
    "href": "slides/intro.html#exhaustive-tracking-of-research-components",
    "title": "Introduction to DataLad",
    "section": "Exhaustive tracking of research components",
    "text": "Exhaustive tracking of research components\n Well-structured datasets (using community standards), and portable computational environments — and their evolution — are the precondition for reproducibility\n\n\n# turn any directory into a dataset\n# with version control\n\n% datalad create &lt;directory&gt;\n\n# save a new state of a dataset with\n# file content of any size\n\n% datalad save"
  },
  {
    "objectID": "slides/intro.html#capture-computational-provenance",
    "href": "slides/intro.html#capture-computational-provenance",
    "title": "Introduction to DataLad",
    "section": "Capture computational provenance",
    "text": "Capture computational provenance\n Which data were needed at which version, as input into which code, running with what parameterization in which computional environment, to generate an outcome?\n\n\n# execute any command and capture its output\n# while recording all input versions too\n\n% datalad run --input ... --output ... &lt;command&gt;"
  },
  {
    "objectID": "slides/intro.html#exhaustive-capture-enables-portability",
    "href": "slides/intro.html#exhaustive-capture-enables-portability",
    "title": "Introduction to DataLad",
    "section": "Exhaustive capture enables portability",
    "text": "Exhaustive capture enables portability\n Precise identification of data and computational environments, combined for provenance records form a comprehensive and portable data structure, capturing all aspects of an investigation.\n\n\n# transfer data and metadata to other sites and services\n# with fine-grained access control for dataset components\n\n% datalad push --to &lt;site-or-service&gt;"
  },
  {
    "objectID": "slides/intro.html#reproducibility-strengthens-trust",
    "href": "slides/intro.html#reproducibility-strengthens-trust",
    "title": "Introduction to DataLad",
    "section": "Reproducibility strengthens trust",
    "text": "Reproducibility strengthens trust\n Outcomes of computational transformations can be validated by authorized 3rd-parties. This enables audits, promotes accountability, and streamlines automated “upgrades” of outputs.\n\n\n# obtain dataset (initially only identity,\n# availability, and provenance metadata)\n\n% datalad clone &lt;url&gt;\n\n# immediately actionable provenance records\n# full abstraction of input data retrieval\n\n% datalad rerun &lt;commit|tag|range&gt;"
  },
  {
    "objectID": "slides/intro.html#ultimate-goal-reusability",
    "href": "slides/intro.html#ultimate-goal-reusability",
    "title": "Introduction to DataLad",
    "section": "Ultimate goal: (re)usability",
    "text": "Ultimate goal: (re)usability\n\nVerifiable, portable, self-contained data structures that track all aspects of an investigation exhaustively can be (re)used as modular components in larger contexts — propagating their traits\n# declare a dependency on another dataset and\n# reuse it at particular state in a new context\n\n% datalad clone -d &lt;superdataset&gt; &lt;path-in-dataset&gt;"
  },
  {
    "objectID": "slides/intro.html#a-meta-view",
    "href": "slides/intro.html#a-meta-view",
    "title": "Introduction to DataLad",
    "section": "A meta view",
    "text": "A meta view"
  },
  {
    "objectID": "slides/intro.html#provider-of-identifiers",
    "href": "slides/intro.html#provider-of-identifiers",
    "title": "Introduction to DataLad",
    "section": "Provider of identifiers",
    "text": "Provider of identifiers\n\nDataLad provides globally unique, persistent identifiers (without a central issuing service; offline and portable)\nConcept identifiers\n\nfor datasets: DataLad dataset ID\nfor files in a dataset: DataLad dataset ID + path within a dataset\n\nContent/version identifiers\n\nfor datasets: Git commit SHA ID\nfor files: Git blob SHA / Git-annex key"
  },
  {
    "objectID": "slides/intro.html#what-is-a-datalad-dataset",
    "href": "slides/intro.html#what-is-a-datalad-dataset",
    "title": "Introduction to DataLad",
    "section": "What is a DataLad dataset?",
    "text": "What is a DataLad dataset?\n\nmetadata on the evolution of a collection of files\n\ncontent identity  think: checksums\ncontent availability  (think: URLs)\nprovenance of change  (think: who did what when?)\n\na regular, but managed directory on computer file system think: provides the familiar look and feel of files and folders\na Git repository  think: compatible with anything that can handle Git repositories\nemploys git-annex for file content tracking and transport think: supports any storage service and transport protocol supported by git-annex\nextends both Git and git-annex with additional features and infrastructure support  think: deposit and retrieve from non-Git-aware sites"
  },
  {
    "objectID": "about/ole.html",
    "href": "about/ole.html",
    "title": "Ole Bialas",
    "section": "",
    "text": "Ole …"
  }
]