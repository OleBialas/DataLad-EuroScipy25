[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "DataLad EuroScipy 2025",
    "section": "",
    "text": "Welcome to the website for the DataLad1 tutorial at EuroScipy 2025! Here, you can access all exercise materials and slides. The rest of this page walks you through the setup process and resources for the tutorial."
  },
  {
    "objectID": "index.html#installation",
    "href": "index.html#installation",
    "title": "DataLad EuroScipy 2025",
    "section": "Installation",
    "text": "Installation\nDataLad requires Git and Python 3.8 or later. All other dependencies can be installed via pip. If you want to learn more about DataLad and its dependencies, check out the handbook.\n\nGit\nTo install and configure git follow these steps:\n\nInstall Git: https://git-scm.com/downloads\nConfigure your git user name: git config --global user.name \"user\"\nConfigure your git user email: git config --global user.email \"user@mail.com\"\n\n\n\nPython\nWe recommend to create a new virtual environment for this project with Python 3.8 or later using a tool like uv, pixi or conda. In your dedicated environment, you can install DataLad and its dependencies via pip: pip install datalad datalad-next git-annex 2. Some of the exercises require you to run Python scripts — you’ll have to install their dependencies as well: pip install pandas seaborn.\n\n\nOther\nWe also recommend to set the following git config, which enables the full functionality of the DataLad-next extension (installed above) by allowing it to override the behavior of the core DataLad package.\n\ngit config --global --add datalad.extensions.load next"
  },
  {
    "objectID": "index.html#data",
    "href": "index.html#data",
    "title": "DataLad EuroScipy 2025",
    "section": "Data",
    "text": "Data\nTo demonstrate DataLad’s data management capabilities, we’ll use a dataset hosted on GIN. You don’t have to download it upfront since cloning the dataset is part of the exercises. The data contains measurements from different penguin species and was originally published by Kristen B Gorman and colleagues 3."
  },
  {
    "objectID": "index.html#further-reading",
    "href": "index.html#further-reading",
    "title": "DataLad EuroScipy 2025",
    "section": "Further Reading",
    "text": "Further Reading\nIf you want to learn more about DataLad, you can check out the handbook which contains lots of beginner-friendly and advanced tutorials as well as the technical documentation which contains detailed descriptions of all DataLad features. If you want to learn more about the underlying file system operations, the git-annex documentation is a useful resource as well."
  },
  {
    "objectID": "index.html#footnotes",
    "href": "index.html#footnotes",
    "title": "DataLad EuroScipy 2025",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nHalchenko et al., (2021). DataLad: distributed system for joint management of code, data, and their relationship. Journal of Open Source Software, 6(63), 3262, https://doi.org/10.21105/joss.03262.↩︎\nGit-annex is not a Python package – it’s written in Haskell, and plenty of installation methods are available – but it is also distributed as a Python wheels package, making for a very convenient way to install as a dependency of DataLad in a virtual environment.↩︎\nGorman, K. B., Williams, T. D., & Fraser, W. R. (2014). Ecological sexual dimorphism and environmental variability within a community of Antarctic penguins (genus Pygoscelis): https://doi.org/10.1371/journal.pone.0090081.↩︎"
  },
  {
    "objectID": "about/michal.html",
    "href": "about/michal.html",
    "title": "Michał Szczepanik",
    "section": "",
    "text": "Michał graduated with BSc and MSc in Neuroinformatics, an interdisciplinary program at the Faculty of Physics, University of Warsaw. This is where he started using Python, as a first-year-student, back in 2011.\nHe completed his PhD at the Laboratory of Brain Imaging (LOBI), Nencki Institute. The PhD was part of the translational project Neural correlates of emotional contagion in humans, which applied fMRI and other methods to study observational fear conditioning. During the time, Michał became particularly interested in the data management side of neuroscience, as LOBI served as a core MRI facility.\nHis current work at the Psychoinformatics Group, INM-7, Forschungszentrum Jülich is in the area of research data management and research software development. It revolves around the data management software DataLad. His responsibilities include bug fixing, user support, technical writing, teaching – and sometmes just trying out new things.\n\nmszczepanik.eu/\n@doktorpanik@masto.ai\norcid.org/0000-0002-4028-2087"
  },
  {
    "objectID": "slides/review1.html#consuming-an-existing-datalad-dataset",
    "href": "slides/review1.html#consuming-an-existing-datalad-dataset",
    "title": "Exercise Review 1",
    "section": "Consuming an Existing DataLad Dataset",
    "text": "Consuming an Existing DataLad Dataset\n\ndatalad clone is fast because it only downloads symbolic file links\ndatalad get downloads the actual file content\nUseful for working on very large datasets:\n\ndatalad clone &lt;dataset&gt;\ndatalad get &lt;file&gt;\npython &lt;script&gt;\ndatalad drop &lt;file&gt;\n\nThis works because DataLad manages the file identifiers that uniquely associate every links with it’s corresponding file content"
  },
  {
    "objectID": "slides/review1.html#a-meta-view",
    "href": "slides/review1.html#a-meta-view",
    "title": "Exercise Review 1",
    "section": "A Meta View",
    "text": "A Meta View\n\n\n\nThis means file identifiers are central to how datalad works so lets take a moment to dive into some of the details here"
  },
  {
    "objectID": "slides/review1.html#provider-and-manager-of-identifiers",
    "href": "slides/review1.html#provider-and-manager-of-identifiers",
    "title": "Exercise Review 1",
    "section": "Provider and Manager of Identifiers",
    "text": "Provider and Manager of Identifiers\n\nDataLad provides globally unique, persistent identifiers (without a central issuing service; offline and portable)\nConcept identifiers (think: where can I find things?)\n\nfor datasets: DataLad dataset ID\nfor files in a dataset: DataLad dataset ID + path within a dataset\n\nContent/version identifiers (think: where can the computer find things?)\n\nfor datasets: Git commit SHA ID\nfor files: Git blob SHA / Git-annex key\n\nBy tracking unique identifiers, DataLad can manage files across multiple sources\n\n\n\nDataLad dataset ID - show cat .datalad/config (there are more configuration options)\nFile identifiers - show git annex info examples/gentoo.jpg\nMutliple sources - show git annex whereis examples/gentoo.jpg"
  },
  {
    "objectID": "slides/review1.html#what-is-a-datalad-dataset",
    "href": "slides/review1.html#what-is-a-datalad-dataset",
    "title": "Exercise Review 1",
    "section": "What is a DataLad Dataset?",
    "text": "What is a DataLad Dataset?\n\na container for metadata on the evolution of a collection of files\n\ncontent identity (think: checksums)\ncontent availability (think: URLs)\nprovenance of change (think: who did what when?)\n\na regular, but managed directory on computer file system\n\nprovides a familiar look and feel of files and folders\n\na Git repository\n\ncompatible with anything that can handle Git repositories\n\na git-annex repository for storing, tracking and transporting file content\n\nsupports any storage service and transport protocol supported by git-annex\n\n\n\nSo now we can answer the question: What actually is a DataLad dataset? -&gt; go through the items So in a nutshell, DataLad integrates and extends Git and git-annex with additional features and infrastructure support. For example, it allows you to deposite datasets to and retrieve them from non-Git-aware sites."
  },
  {
    "objectID": "slides/review1.html#hands-on-tracking-changes",
    "href": "slides/review1.html#hands-on-tracking-changes",
    "title": "Exercise Review 1",
    "section": "Hands-on: Tracking Changes",
    "text": "Hands-on: Tracking Changes\n\nOpen the Exercises Part 2 on the tutorial website 📖\n\n\n\n\n\nLet’s move to out second set of exercises!\nIn these exercises, you’ll be able to see for your selfes how DataLad tracks the evolution of a dataset\nYou’ll also explore a very useful feature of DataLad which is it’s ability to run and rerun code while keeping track of the inputs and outputs which implicitly builds a pipeline for your data analysis"
  },
  {
    "objectID": "slides/review3.html#using-a-bare-git-repository-as-push-target",
    "href": "slides/review3.html#using-a-bare-git-repository-as-push-target",
    "title": "Exercise Review 3",
    "section": "Using a Bare Git Repository as Push Target",
    "text": "Using a Bare Git Repository as Push Target\n\nbare == no worktree\n\ncontents of .git directly in the directory\nwhen shared pushes can’t break the worktree\ncan be put on a server 1\n\nwith git-annex on the machine, can store git+annex\ngood default for Git & git-annex aware services\n\n\nDemo: git init --bare ~/penguins_backup\n\nhttps://git-scm.com/book/en/v2/Git-on-the-Server-Getting-Git-on-a-Server"
  },
  {
    "objectID": "slides/review3.html#what-contents-are-actually-being-pushed",
    "href": "slides/review3.html#what-contents-are-actually-being-pushed",
    "title": "Exercise Review 3",
    "section": "What Contents Are Actually being pushed?",
    "text": "What Contents Are Actually being pushed?\n\nAny git repository (local or online) can be added as a dataset sibling\nThe first push initializes the repository’s annex ID, the second push actually tranfers the files\ndatalad push acts on the contents you already got\n\nto transfer specific files, use datalad get first\n\ngit annex copy [--from=remote|--to=remote] to download and upload (no magic)\n\n\n\nSibling demo: datalad siblings add --name backup --url ~/penguins_backup\npush demo: datalad push --to backup (twice)\nNote that DataLad will only push the files that are present in the current repository. For files that are not present, we have to use datalad get first\nAlternatively, you can use git annex copy which does the download and upload in one step"
  },
  {
    "objectID": "slides/review3.html#git-annex-special-remotes",
    "href": "slides/review3.html#git-annex-special-remotes",
    "title": "Exercise Review 3",
    "section": "git-annex Special Remotes",
    "text": "git-annex Special Remotes\n\nstore and retrieve file content (not Git repo)1\ngit annex initremote ...\nbuilt-in and external implementations\n\nhttps://git-annex.branchable.com/special_remotes/\n\n\nbut actually also Git repo bundles, with git-remote-annex"
  },
  {
    "objectID": "slides/review3.html#rd-party-services",
    "href": "slides/review3.html#rd-party-services",
    "title": "Exercise Review 3",
    "section": "3rd Party Services",
    "text": "3rd Party Services\n\n\nDataLad supports a variety of third party services - some are by default, others via a dedicated extension. In the DataLad handbook (which is linked on our website), you’ll find tutorials for many different storage services. In the bonus exercises for this notebook you can explore hosting your dataset on GIN which is the service we used for the penguins dataset"
  },
  {
    "objectID": "slides/review3.html#distributed-data-management",
    "href": "slides/review3.html#distributed-data-management",
    "title": "Exercise Review 3",
    "section": "Distributed Data Management",
    "text": "Distributed Data Management\n\nrecall git annex whereis (n copies)\nkeyword: distributed data management\n\ncan have multiple special remotes\n\ngit annex configuration:\n\npreferred content\nnumcopies"
  },
  {
    "objectID": "notebooks/exercises2.html",
    "href": "notebooks/exercises2.html",
    "title": "Part 2: Tracking Changes in DataLad Datasets",
    "section": "",
    "text": "DataLad keeps track of all changes made to your dataset. In this section, you will add new content to the penguins dataset and see how these changes are tracked in the git log of your repository.\n\n\n\n\n\n\n\nCommand\nDescription\n\n\n\n\ndatalad status\nShow any untracked changes in the current dataset\n\n\ndatalad save\nSave any untracked changes in the current dataset\n\n\ndatalad save -m \"hi\"\nSave untracked changes and add the message \"hi\"\n\n\ndatalad unlock file.txt\nUnlock file.txt to make it modifiable\n\n\ngit log\nView the dataset’s history, stored in the git log\n\n\ngit log -3\nView the last 3 entries in the git log\n\n\n\n\nExercise 1 Create a new file in the penguins folder called penguin_species.txt and add the species names gentoo and adelie. Then, save the file and run datalad status to see the untracked changes.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nOn Linux/macOS:\necho -e \"gentoo\\nadelie\" &gt; penguin_species.txt\ndatalad status\nOn Windows:\n(echo gentoo & echo adelie) &gt; penguin_species.txt\nor just use a text editor of your choice.\n\n\n\n\nExercise 2 Use datalad save to save the untracked changes with the message \"list penguin species\".\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\ndatalad save -m \"list penguin species\"\n\n\n\n\nExercise 3 Open the git log and find the entry created by the previous datalad save command\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nOn Linux/macOS this should be the last (i.e. top) entry, on Windows it will be the second-to-last.\ngit log\n\n\n\n\nExercise 4 Use datalad unlock to unlock the penguin_species.txt file and append chinstrap to the list. Then, run datalad save again with a message to save the changes\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nOn Linux/macOS:\ndatalad unlock penguin_species.txt\necho -e \"chinstrap\" &gt;&gt; penguin_species.txt\ndatalad save -m \"add chinstrap\"\nOn Windows:\ndatalad unlock penguin_species.txt\necho chinstrap &gt;&gt; penguin_species.txt\ndatalad save -m \"add chinstrap\"\n\n\n\n\nExercise 5 Open the git log again and find the entry from the datalad save command above.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\ngit log"
  },
  {
    "objectID": "notebooks/exercises2.html#modifying-a-dataset",
    "href": "notebooks/exercises2.html#modifying-a-dataset",
    "title": "Part 2: Tracking Changes in DataLad Datasets",
    "section": "",
    "text": "DataLad keeps track of all changes made to your dataset. In this section, you will add new content to the penguins dataset and see how these changes are tracked in the git log of your repository.\n\n\n\n\n\n\n\nCommand\nDescription\n\n\n\n\ndatalad status\nShow any untracked changes in the current dataset\n\n\ndatalad save\nSave any untracked changes in the current dataset\n\n\ndatalad save -m \"hi\"\nSave untracked changes and add the message \"hi\"\n\n\ndatalad unlock file.txt\nUnlock file.txt to make it modifiable\n\n\ngit log\nView the dataset’s history, stored in the git log\n\n\ngit log -3\nView the last 3 entries in the git log\n\n\n\n\nExercise 1 Create a new file in the penguins folder called penguin_species.txt and add the species names gentoo and adelie. Then, save the file and run datalad status to see the untracked changes.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nOn Linux/macOS:\necho -e \"gentoo\\nadelie\" &gt; penguin_species.txt\ndatalad status\nOn Windows:\n(echo gentoo & echo adelie) &gt; penguin_species.txt\nor just use a text editor of your choice.\n\n\n\n\nExercise 2 Use datalad save to save the untracked changes with the message \"list penguin species\".\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\ndatalad save -m \"list penguin species\"\n\n\n\n\nExercise 3 Open the git log and find the entry created by the previous datalad save command\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nOn Linux/macOS this should be the last (i.e. top) entry, on Windows it will be the second-to-last.\ngit log\n\n\n\n\nExercise 4 Use datalad unlock to unlock the penguin_species.txt file and append chinstrap to the list. Then, run datalad save again with a message to save the changes\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nOn Linux/macOS:\ndatalad unlock penguin_species.txt\necho -e \"chinstrap\" &gt;&gt; penguin_species.txt\ndatalad save -m \"add chinstrap\"\nOn Windows:\ndatalad unlock penguin_species.txt\necho chinstrap &gt;&gt; penguin_species.txt\ndatalad save -m \"add chinstrap\"\n\n\n\n\nExercise 5 Open the git log again and find the entry from the datalad save command above.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\ngit log"
  },
  {
    "objectID": "notebooks/exercises2.html#running-scripts-with-datalad",
    "href": "notebooks/exercises2.html#running-scripts-with-datalad",
    "title": "Part 2: Tracking Changes in DataLad Datasets",
    "section": "Running Scripts with DataLad",
    "text": "Running Scripts with DataLad\nOften, we won’t edit our dataset manually but run scripts that do so. In this section you will use DataLad to run Python scripts and track the changes made by them. You are also going to use the dataset’s history to re-run the commands.\n\n\n\n\n\n\n\nCommand\nDescription\n\n\n\n\ndatalad run \"python script.py\"\nRun the python script script.py\n\n\ndatalad run --input \"data.csv\" --output \"figure.png\" \"python script.py\"\nRun script.py with input \"data.csv\" and output \"figure.png\"\n\n\ngit log\nView the dataset’s history stored in the git log\n\n\ndatalad rerun a268d8ca22b6\nRerun the command from the git log with the checksum starting with a268d8ca22b6e87959\n\n\ndatalad rerun --since a268d8ca22b6\nRerun ALL commands --since the one with the checksum starting with a268d8ca22b6e87959\n\n\n\n\nExercise 6 Try to run the python script in code/aggregate_culmen_data.py. What error message do you observe?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\ndatalad run \"python code/aggregate_culmen_data.py\"\nCurrently, the dataset does not contain the annexed content for the required files. On Linux/macOS, this will result in a FileNotFoundError: [Errno 2] No such file or directory On Windows, you’ll see KeyError: \"None of [Index(['Culmen Length (mm)', 'Culmen Depth (mm)', 'Species'], dtype='object')] are in the [columns]\" because Python will actually open the pointer file and crash because it can’t find the required data.\n\n\n\n\nExercise 7 Run the same script with the data/ folder as --input and the file \"results/penguin_culmens.csv\" as --output.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\ndatalad run --input \"data/\" --output \"results/penguin_culmens.csv\" \"python code/aggregate_culmen_data.py\"\n\n\n\n\nExercise 8 Open the git log to view the entry created by the datalad run command. Then, copy the checksum of that commit\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\ngit log\nThe git log entry should look like this:\ncommit af78031c9ca45d3c349a692b0afd332178639e64 (master)\nAuthor: Ole Bialas &lt;ole.bialas@posteo.de&gt;\nDate:   Thu Jul 31 11:14:00 2025 +0200\n\n    [DATALAD RUNCMD] python code/aggregate_culmen_data.py\n\n    === Do not change lines below ===\n    {\n     \"chain\": [],\n     \"cmd\": \"python code/aggregate_culmen_data.py\",\n     \"dsid\": \"3a8aacc5-85f0-4114-adee-fcfa7d21a5df\",\n     \"exit\": 0,\n     \"extra_inputs\": [],\n     \"inputs\": [\n      \"data\"\n     ],\n     \"outputs\": [\n      \"results/penguin_culmens.csv\"\n     ],\n     \"pwd\": \".\"\n    }\n    ^^^ Do not change lines above ^^^\nThe checksum is displayed on the first line, after “commit”: af78031c9ca45d3c349a692b0afd332178639e64\n\n\n\n\nExercise 9 Use the copied checksum to rerun the previous datalad run command\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe command looks like this but the checksum will be different for everyone:\ndatalad rerun af78031c9ca45d3c349a692b0afd332178639e64\n\n\n\n\nExercise 10 Run the script code/plot_culmen_length_vs_depth.py — it takes results/penguin_culmens.csv as --input and produces results/culmen_length_vs_depth.png as an output.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\ndatalad run --input \"results/penguin_culmens.csv\" --output \"results/culmen_length_vs_depth.png\" \"python code/plot_culmen_length_vs_depth.py\"\n\n\n\n\nExercise 11 Use the checksum of the very first commit (that says [DATALAD] new dataset) to re-run everything --since this commit (i.e. the whole analysis).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\ndatalad rerun --since 4c2b9dcd5c745b519095f7bf6612bbe20f7ae9bb"
  },
  {
    "objectID": "notebooks/exercises2.html#further-reading",
    "href": "notebooks/exercises2.html#further-reading",
    "title": "Part 2: Tracking Changes in DataLad Datasets",
    "section": "Further reading",
    "text": "Further reading\n\nFor more on DataLad run and the comparison of how Git and git-annex handle files, see these chapters of the DataLad Handbook:\n\nDataLad, run!\nUnder the hood: git-annex\n\nFor even more on git-annex under the hood, see git-annex documentation:\n\nUnlocked files\nPointer files\nAdjusted branches"
  },
  {
    "objectID": "notebooks/exercises1.html",
    "href": "notebooks/exercises1.html",
    "title": "Part 1: Working with DataLad Datasets",
    "section": "",
    "text": "In this section, you are going to clone an existing DataLad dataset and download its contents. While the datalad API is universal, the commands for navigating the data set differ between operating systems (see table below).\n\nTerminal commands\n\n\n\n\n\n\n\nLinux/macOS\nWindows\nDescription\n\n\n\n\nls -a\ndir /a\nList the content of the current directory (including hidden files)\n\n\nls -a data\ndir /a data\nList the content of the data directory\n\n\ndu -sh\ndir /s\nGet the disk usage of the current directory\n\n\ndu -sh data\ndir /s data\nGet the disk usage of the data directory\n\n\ncd data\ncd data\nChange the directory to data\n\n\n\n\nDataLad commands\n\n\n\n\n\n\nCommand\nDescription\n\n\n\n\ndatalad clone https://example.com\nClone the data set from example.com\n\n\ndatalad get folder/\nGet the file content of the folder/\n\n\ndatalad get folder/image.png\nGet the file content of the file image.png\n\n\ndatalad drop folder/\nDrop the file content of the folder/\n\n\n\nOpen a terminal to do the following exercises. For Windows users, we recommend CMD (the solutions will assume you are using CMD).\n\nExercise 1 Clone the dataset from https://gin.g-node.org/obi/penguins\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\ndatalad clone https://gin.g-node.org/obi/penguins\n\n\n\n\nExercise 2 Change the directory to penguins and list the directory’s content\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nOn Linux/macOS:\ncd penguins\nls -a\nOn Windows:\ncd penguins\ndir /a\n\n\n\n\nExercise 3 Check the disk usage of the penguins directory\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nOn Linux/macOS:\ndu -sh\nOn Windows:\ndir /s\n\n\n\n\nExercise 4 Get the content of the examples subdirectory\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\ndatalad get examples\n\n\n\n\nExercise 5 Check the disk usage of the penguins directory again\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nOn Linux/macOS:\ndu -sh\nOn Windows:\ndir /s\n\n\n\n\nExercise 6 Drop the content of examples/chinstrap.jpg and check the disk usage again\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\ndatalad drop examples/chinstrap.jpg\nOn Linux/macOS:\ndu -sh\nOn Windows:\ndir /s"
  },
  {
    "objectID": "notebooks/exercises1.html#consuming-existing-datasets",
    "href": "notebooks/exercises1.html#consuming-existing-datasets",
    "title": "Part 1: Working with DataLad Datasets",
    "section": "",
    "text": "In this section, you are going to clone an existing DataLad dataset and download its contents. While the datalad API is universal, the commands for navigating the data set differ between operating systems (see table below).\n\nTerminal commands\n\n\n\n\n\n\n\nLinux/macOS\nWindows\nDescription\n\n\n\n\nls -a\ndir /a\nList the content of the current directory (including hidden files)\n\n\nls -a data\ndir /a data\nList the content of the data directory\n\n\ndu -sh\ndir /s\nGet the disk usage of the current directory\n\n\ndu -sh data\ndir /s data\nGet the disk usage of the data directory\n\n\ncd data\ncd data\nChange the directory to data\n\n\n\n\nDataLad commands\n\n\n\n\n\n\nCommand\nDescription\n\n\n\n\ndatalad clone https://example.com\nClone the data set from example.com\n\n\ndatalad get folder/\nGet the file content of the folder/\n\n\ndatalad get folder/image.png\nGet the file content of the file image.png\n\n\ndatalad drop folder/\nDrop the file content of the folder/\n\n\n\nOpen a terminal to do the following exercises. For Windows users, we recommend CMD (the solutions will assume you are using CMD).\n\nExercise 1 Clone the dataset from https://gin.g-node.org/obi/penguins\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\ndatalad clone https://gin.g-node.org/obi/penguins\n\n\n\n\nExercise 2 Change the directory to penguins and list the directory’s content\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nOn Linux/macOS:\ncd penguins\nls -a\nOn Windows:\ncd penguins\ndir /a\n\n\n\n\nExercise 3 Check the disk usage of the penguins directory\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nOn Linux/macOS:\ndu -sh\nOn Windows:\ndir /s\n\n\n\n\nExercise 4 Get the content of the examples subdirectory\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\ndatalad get examples\n\n\n\n\nExercise 5 Check the disk usage of the penguins directory again\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nOn Linux/macOS:\ndu -sh\nOn Windows:\ndir /s\n\n\n\n\nExercise 6 Drop the content of examples/chinstrap.jpg and check the disk usage again\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\ndatalad drop examples/chinstrap.jpg\nOn Linux/macOS:\ndu -sh\nOn Windows:\ndir /s"
  },
  {
    "objectID": "notebooks/exercises1.html#checking-file-identity-and-location-with-git-annex",
    "href": "notebooks/exercises1.html#checking-file-identity-and-location-with-git-annex",
    "title": "Part 1: Working with DataLad Datasets",
    "section": "Checking File Identity and Location with git-annex",
    "text": "Checking File Identity and Location with git-annex\nSince DataLad is built on top of git-annex, you can use its commands on any DataLad dataset. In this section, you’ll use git-annex to get information on the dataset and locate its file contents.\n\n\n\n\n\n\n\nCommand\nDescription\n\n\n\n\ngit annex info\nShow the git-annex information for the whole dataset\n\n\ngit annex info folder/image.png\nShow the git-annex information for the file image.png\n\n\ngit annex whereis folder/image.png\nList the repositories that have the file content for image.png\n\n\n\n\nExercise 7 Display the git annex info for the file examples/gentoo.jpg. What is the size of that file? Is it present on your machine?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\ngit annex info examples/gentoo.jpg\nThe file is 4.81 megabtyes and it should be present since we previously loaded the content of the examples folder.\n\n\n\n\nExercise 8 Display the git-annex info of the whole data set. How many annexed files are there in the working tree?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\ngit annex info\nThe number of annexed files is displayed in this line: annexed files in working tree: 6\n\n\n\n\nExercise 9 Use git annex whereis to list the repositories that have the file content for the image examples/gentoo.jpg.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\ngit annex whereis examples/gentoo.jpg\n\n\n\n\nExercise 10 Use git annex whereis to list the repositories that have the file content for the table data/table_220.csv. How does this differ from the list of repositories that contain the content for gentoo.jpg?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\ngit annex whereis data/table_220.csv\nThe table is not stored in the local repository, listed in the line marked [here]."
  },
  {
    "objectID": "notebooks/exercises3.html",
    "href": "notebooks/exercises3.html",
    "title": "Part 3: Creating Backups and Sharing DataLad Datasets",
    "section": "",
    "text": "Command\nDescription\n\n\n\n\ngit init --bare ~/mydir\nCreate a --bare repository called mydir in the home directory (on Linux/macOS)\n\n\ngit init --bare %USERPROFILE%\\mydir\nCreate a --bare repository called mydir in the home directory (on Windows / CMD)\n\n\ngit init --bare \"$env:USERPROFILE\\mydir\"\nCreate a --bare repository called mydir in the home directory (on Windows / PowerShell)\n\n\ndatalad siblings\nList all siblings of the current dataset\n\n\ndatalad sibings add --name new --url ~/mydir\nAdd the repository at ~/mydir as a new sibling with the name new\n\n\ndatalad push --to new\nPush the dataset content to the sibling named new\n\n\n\n\nExercise 1 List all siblings of the current dataset.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\ndatalad siblings\n\n\n\n\nExercise 2 Initialize a --bare git repository at a path outside of this dataset.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nOn Linux/macOS\ngit init --bare ~/penguins_backup\nOn Windows\ngit init --bare %USERPROFILE%\\penguins_backup\n\n\n\n\nExercise 3 add a new sibling to the dataset using the path to the newly created git repository as the --url. Then, list all siblings to confirm it was added.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nOn Linux/macOS\ndatalad siblings add --name backup --url ~/penguins_backup\ndatalad siblings\nOn Windows\ndatalad siblings add --name backup --url %USERPROFILE%\\penguins_backup\ndatalad siblings\n\n\n\n\nExercise 4 Push the dataset to the new sibling twice.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nWe need to push tiwce because the first push initializes the repository’s annex ID and the second (and each subsequent) push actually tranfer the annexed files.\ndatalad push --to backup\ndatalad push --to backup\n\n\n\n\nExercise 5 Move to a directory outside of this dataset and clone the new sibling dataset.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nOn Linux/macOS\ncd ..\ndatalad clone ~/penguins_backup\nOn Windows\ndatalad clone %USERPROFILE%\\penguins_backup"
  },
  {
    "objectID": "notebooks/exercises3.html#creating-a-backup",
    "href": "notebooks/exercises3.html#creating-a-backup",
    "title": "Part 3: Creating Backups and Sharing DataLad Datasets",
    "section": "",
    "text": "Command\nDescription\n\n\n\n\ngit init --bare ~/mydir\nCreate a --bare repository called mydir in the home directory (on Linux/macOS)\n\n\ngit init --bare %USERPROFILE%\\mydir\nCreate a --bare repository called mydir in the home directory (on Windows / CMD)\n\n\ngit init --bare \"$env:USERPROFILE\\mydir\"\nCreate a --bare repository called mydir in the home directory (on Windows / PowerShell)\n\n\ndatalad siblings\nList all siblings of the current dataset\n\n\ndatalad sibings add --name new --url ~/mydir\nAdd the repository at ~/mydir as a new sibling with the name new\n\n\ndatalad push --to new\nPush the dataset content to the sibling named new\n\n\n\n\nExercise 1 List all siblings of the current dataset.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\ndatalad siblings\n\n\n\n\nExercise 2 Initialize a --bare git repository at a path outside of this dataset.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nOn Linux/macOS\ngit init --bare ~/penguins_backup\nOn Windows\ngit init --bare %USERPROFILE%\\penguins_backup\n\n\n\n\nExercise 3 add a new sibling to the dataset using the path to the newly created git repository as the --url. Then, list all siblings to confirm it was added.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nOn Linux/macOS\ndatalad siblings add --name backup --url ~/penguins_backup\ndatalad siblings\nOn Windows\ndatalad siblings add --name backup --url %USERPROFILE%\\penguins_backup\ndatalad siblings\n\n\n\n\nExercise 4 Push the dataset to the new sibling twice.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nWe need to push tiwce because the first push initializes the repository’s annex ID and the second (and each subsequent) push actually tranfer the annexed files.\ndatalad push --to backup\ndatalad push --to backup\n\n\n\n\nExercise 5 Move to a directory outside of this dataset and clone the new sibling dataset.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nOn Linux/macOS\ncd ..\ndatalad clone ~/penguins_backup\nOn Windows\ndatalad clone %USERPROFILE%\\penguins_backup"
  },
  {
    "objectID": "notebooks/exercises3.html#bonus-sharing-your-dataset-online",
    "href": "notebooks/exercises3.html#bonus-sharing-your-dataset-online",
    "title": "Part 3: Creating Backups and Sharing DataLad Datasets",
    "section": "BONUS: Sharing your Dataset online",
    "text": "BONUS: Sharing your Dataset online\n\n\n\n\n\n\n\nCommand\nDescription\n\n\n\n\nssh-keygen\nGenerate a public and private authentication key pair\n\n\ndatalad siblings\nList all siblings of the current dataset\n\n\ndatalad sibings add --name gin --url git@gin.g-node.org:/user/repo.git\nAdd the gin repository at /https://gin.g-node.org/user/repo as a new sibling with the name gin\n\n\ndatalad push --to gin\nPush the dataset content to the sibling named gin\n\n\n\n\nExercise 6 Use ssh-keygen to generate a public and private key pair (you don’t have to use a passphrase). Note the location where the public key is stored, e.g. .ssh/id_ed25519.pub. Open the .pub file and copy the whole content — it should look something like this: ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIBOYcoRKZZLWA4FWECpW2K/fTOvuRYXBnBA6gcea2bFq &lt;user&gt;@&lt;computer&gt;\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nssh-keygen\n\n\n\n\nExercise 7 Login in to your GIN account, go to your user settings and add the copied ssh key. Now datalad should be able to connect to your GIN account! \n\n\nExercise 8 Create a new repository on GIN, make sure to NOT initialize it with a README. \n\n\nExercise 9 add a new sibling to the dataset using the --url of the newly created gin repository and confirm the connection. Then, list all siblings to confirm it was added.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nFor the repository in the image above, the command would look like this:\ndatalad siblings add --name gin --url git@gin.g-node.org:/adswa/DataLad-101.git\n\n\n\n\nExercise 10 Push the dataset to the new GIN sibling. Then, open the repository in your browser to confirm the content was pushed.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\ndatalad push --to gin\n\n\n\n\nExercise 11 Move to a directory outside of this dataset and clone the new GIN sibling.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nFor the repository in the image above, the command would look like this:\ncd ..\ndatalad clone datalad clone https://gin.g-node.org/adswa/DataLad-101"
  },
  {
    "objectID": "notebooks/exercises3.html#further-reading",
    "href": "notebooks/exercises3.html#further-reading",
    "title": "Part 3: Creating Backups and Sharing DataLad Datasets",
    "section": "Further reading",
    "text": "Further reading\nIn the examples above, the annex was published together with the Git repository. However, this is a bit of a special case, and in many scenarios they can be moved separately. For an overview and examples of several different publishing scenarios, see the Beyond shared infrastructure chapter of the DataLad handbook.\nGit-annex supports multiple options for publishing file contents; see the list of built-in special remotes. And for a very special case, in which the Git repository is placed by git-annex in a non-git-aware hosting, see git-remote annex.\nFinally, Forgejo is gaining popularity as a self-hosted software forge. Forgejo-aneksajo is a soft fork of Forgejo which adds git-annex capability. See also Collaborative infrastructure for a lab: Forgejo on the DataLad blog .s"
  },
  {
    "objectID": "slides/review2.html#git-versus-git-annex",
    "href": "slides/review2.html#git-versus-git-annex",
    "title": "Exercise Review 2",
    "section": "Git versus Git-annex",
    "text": "Git versus Git-annex\n\nData in datasets is either stored in Git or git-annex\nMatter of configuration; by default, everything is annexed"
  },
  {
    "objectID": "slides/review2.html#git-versus-git-annex-1",
    "href": "slides/review2.html#git-versus-git-annex-1",
    "title": "Exercise Review 2",
    "section": "Git versus Git-annex",
    "text": "Git versus Git-annex\nGit and git-annex handle files differently:\n\nFiles in Git are downloaded during clone, annexed contents are retrieved on demand (get).\nContent identity and availability information of annexed files are available after cloning.\nOn datalad save, annexed contents are hashed, moved to .git/annex/objects and symlinked.\nGit versions the symlink (content-identity), not the content.\nContent is “locked” (write-protected) against accidental modifications.\nFiles stored in Git are modifiable, annexed files are write-protected."
  },
  {
    "objectID": "slides/review2.html#annexed-files-on-windows",
    "href": "slides/review2.html#annexed-files-on-windows",
    "title": "Exercise Review 2",
    "section": "Annexed files on Windows",
    "text": "Annexed files on Windows\nWindows’ file system does not support symlinks1. Under these conditions, git-annex automatically operates in “adjusted unlocked mode”:\n\nFile contents are duplicated: One copy to edit, one to keep safe.\nNo (un)locking – trade-off: disk space use vs easy modification.\nAnnexed files are pointer files instead of symlinks: A text file with the contents “/annex/objects/” followed by the key.\nGit annex uses adjusted branches, which unlock on top the locked counterpart branch. Compatibility with other systems but confusing Git log and issues in some nesting usecases.\n\n\n\nOversimplification. It kind-of does, but not by default."
  },
  {
    "objectID": "slides/intro.html#about-us",
    "href": "slides/intro.html#about-us",
    "title": "Introduction to DataLad",
    "section": "About us",
    "text": "About us\n\n\n\n\n\n\nOle\n\n\n\n\nPhD in neuroscience researching auditory perception\nResearch software consultant at the University of Bonn\nDataLad user for ~5 years\n\n\n\n\n\n\n\n\n\nMichał\n\n\n\n\nPhD in (cognitive) neuroscience – emotional contagion\nRDM / RSE at the Forschungszentrum Jülich\nDataLad contributor since ~4 years\n\n\n\n\n\nMy name is Ole, I’m a neuroscientist by training, working as a research software consultant at the University of Bonn and I have been using DataLad for several years in my own research projects\nWith me is Michal who is also a neuroscientist and part of the DataLad development team at the research center in Juelich"
  },
  {
    "objectID": "slides/intro.html#resources",
    "href": "slides/intro.html#resources",
    "title": "Introduction to DataLad",
    "section": "Resources",
    "text": "Resources\n\nWebsite: olebialas.github.io/DataLad-EuroScipy25\nContains installation instructions, slides and exercises\n\n\n\n\n\n\nThere is a website for this tutorial which you can find via this URL or the QR code you can see on this slide\nOn the landing page, you’ll find the installation instuctions so if you haven’t already please follow them to set up your environment\nYou’ll also find all exercises and slides for the tutorial. For example, to access this presentation go to slides &gt; introduction"
  },
  {
    "objectID": "slides/intro.html#a-community-project",
    "href": "slides/intro.html#a-community-project",
    "title": "Introduction to DataLad",
    "section": "A community project",
    "text": "A community project\n\n10+ years of ongoing development & maintenance 1\n100+ contributors across core, extensions, and Handbook :\nstarted by:\n\nMichael Hanke (now: Psychoinformatics Lab, Forschungszentrum Jülich)\nYaroslav Halchenko (now: Center for Open Neuroscience, Dartmouth College)\n\nowes a lot to git-annex by Joey Hess & contributors\n\nhttps://doi.org/10.34734/FZJ-2025-01847"
  },
  {
    "objectID": "slides/intro.html#a-piece-of-software",
    "href": "slides/intro.html#a-piece-of-software",
    "title": "Introduction to DataLad",
    "section": "A piece of software",
    "text": "A piece of software\n\nSoftware for data management\nWritten in Python\nBased on git and git-annex\nFOSS (MIT license)"
  },
  {
    "objectID": "slides/intro.html#exhaustive-tracking-of-research-components",
    "href": "slides/intro.html#exhaustive-tracking-of-research-components",
    "title": "Introduction to DataLad",
    "section": "Exhaustive tracking of research components",
    "text": "Exhaustive tracking of research components\n Well-structured datasets (using community standards), and portable computational environments — and their evolution — are the precondition for reproducibility\n\n\n# turn any directory into a dataset\n# with version control\n\n% datalad create &lt;directory&gt;\n\n# save a new state of a dataset with\n# file content of any size\n\n% datalad save"
  },
  {
    "objectID": "slides/intro.html#capture-computational-provenance",
    "href": "slides/intro.html#capture-computational-provenance",
    "title": "Introduction to DataLad",
    "section": "Capture computational provenance",
    "text": "Capture computational provenance\n Which data were needed at which version, as input into which code, running with what parameterization in which computional environment, to generate an outcome?\n\n\n# execute any command and capture its output\n# while recording all input versions too\n\n% datalad run --input ... --output ... &lt;command&gt;"
  },
  {
    "objectID": "slides/intro.html#exhaustive-capture-enables-portability",
    "href": "slides/intro.html#exhaustive-capture-enables-portability",
    "title": "Introduction to DataLad",
    "section": "Exhaustive capture enables portability",
    "text": "Exhaustive capture enables portability\n Precise identification of data and computational environments, combined for provenance records form a comprehensive and portable data structure, capturing all aspects of an investigation.\n\n\n# transfer data and metadata to other sites and services\n# with fine-grained access control for dataset components\n\n% datalad push --to &lt;site-or-service&gt;"
  },
  {
    "objectID": "slides/intro.html#reproducibility-strengthens-trust",
    "href": "slides/intro.html#reproducibility-strengthens-trust",
    "title": "Introduction to DataLad",
    "section": "Reproducibility strengthens trust",
    "text": "Reproducibility strengthens trust\n Outcomes of computational transformations can be validated by authorized 3rd-parties. This enables audits, promotes accountability, and streamlines automated “upgrades” of outputs.\n\n\n# obtain dataset (initially only identity,\n# availability, and provenance metadata)\n\n% datalad clone &lt;url&gt;\n\n# immediately actionable provenance records\n# full abstraction of input data retrieval\n\n% datalad rerun &lt;commit|tag|range&gt;"
  },
  {
    "objectID": "slides/intro.html#ultimate-goal-reusability",
    "href": "slides/intro.html#ultimate-goal-reusability",
    "title": "Introduction to DataLad",
    "section": "Ultimate goal: (re)usability",
    "text": "Ultimate goal: (re)usability\n\nVerifiable, portable, self-contained data structures that track all aspects of an investigation exhaustively can be (re)used as modular components in larger contexts — propagating their traits\n# declare a dependency on another dataset and\n# reuse it at particular state in a new context\n\n% datalad clone -d &lt;superdataset&gt; &lt;path-in-dataset&gt;"
  },
  {
    "objectID": "slides/intro.html#hands-on-working-with-a-datalad-dataset",
    "href": "slides/intro.html#hands-on-working-with-a-datalad-dataset",
    "title": "Introduction to DataLad",
    "section": "Hands-on: Working with a DataLad Dataset",
    "text": "Hands-on: Working with a DataLad Dataset\n\nOpen the Exercises Part 1 on the tutorial website 📖\nEach section starts with a table that contains all required commands 💻\nFeel free to chat with your neighbor and check the solutions 💡\n\n\n\n\n\nLet’s move to out first set of exercises!\nYou can find them at Exercises &gt; Part 1\nEach section starts with a table that lists all required commands\nBelow every exercise there is an expandable box with the solution you can check\nDon’t worry if you don’t complete all exercises, this is not required for the rest of the tutorial and you can always come back to the website\nIf you have andy questions or problems during the exercises just raise your hand and we’ll be with you"
  },
  {
    "objectID": "slides/intro.html#hands-on-working-with-a-datalad-dataset-1",
    "href": "slides/intro.html#hands-on-working-with-a-datalad-dataset-1",
    "title": "Introduction to DataLad",
    "section": "Hands-on: Working with a DataLad Dataset",
    "text": "Hands-on: Working with a DataLad Dataset\n                    \n                    \n                \n\n\nWebsite: olebialas.github.io/DataLad-EuroScipy25\nGo to: Exercises &gt; Part1: Working with DataLad Datasets"
  },
  {
    "objectID": "about/ole.html",
    "href": "about/ole.html",
    "title": "Ole Bialas",
    "section": "",
    "text": "I studied Biology at the University of Tübingen, where I first learned how to code using Matlab. Then, I moved to Leipzig, where I did a master’s degree and later a PhD in neurobiology. In my research, I studied how the brain processes sound location using electroencephalography (EEG) and custom experimental setups for spatial audio. During that time, I started using Python and eventually co-authored slab, a Python toolbox for psychoacoustic experiments. After my PhD, I moved to the University of Rochester in New York, where I studied how the brain processes naturalistic speech by modeling EEG that was recorded while the participants listened to audiobooks. For this research I published another toolbox, originally written in Matlab, called mTRFpy. As my postdoc was coming to an end, I was looking for a position where I could combine my interest in neuroscience with my passion for programming. I found such a position at the iBehave Open Technology Support iBOTS where I currently work as a research software consultant. In this position, I develop and teach workshops where neuroscience researchers can improve their software skills and I do one-on-one consulting to help neuroscientists deal with the computational challenges they are faced in their research."
  }
]