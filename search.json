[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "DataLad EuroScipy 2025",
    "section": "",
    "text": "Welcome to the website for the DataLad1 tutorial at EuroScipy 2025! Here, you can access all exercise materials and slides. The rest of this page walks you through the setup process and resources for the tutorial."
  },
  {
    "objectID": "index.html#installation",
    "href": "index.html#installation",
    "title": "DataLad EuroScipy 2025",
    "section": "Installation",
    "text": "Installation\nDataLad requires Git and Python 3.8 or later. All other dependencies can be installed via pip. if you want to learn more about DataLad and its dependencies, check out the handbook\n\nGit\nTo install and configure git follow these steps: - Install git https://git-scm.com/downloads - Configure your git user name: git config --global user.name \"user\" - Configure your git user email: git config --global user.email \"user@mail.com\"\n\n\nPython\nWe recommend to create a new virtual environment for this project with Python 3.8 or later using a tool like uv, pixi or conda. In your dedicated environment, you can install DataLad and its dependencies via pip: pip install datalad datalad-next git-annex Some of the exercises require you to run Python scripts — you’ll have to install their dependencies as well: pip install pandas seaborn"
  },
  {
    "objectID": "index.html#data",
    "href": "index.html#data",
    "title": "DataLad EuroScipy 2025",
    "section": "Data",
    "text": "Data\nTo demonstrate DataLad’s data management capabilities, we’ll use a dataset hosted on GIN. You don’t have to download it upfront since cloning the dataset is part of the exercises. The data contains measurements from different penguin species and was originally published by Kristen B Gorman and colleagues 2."
  },
  {
    "objectID": "index.html#further-reading",
    "href": "index.html#further-reading",
    "title": "DataLad EuroScipy 2025",
    "section": "Further Reading",
    "text": "Further Reading\nIf you want to learn more about DataLad, you can check out the handbook which contains lots of beginner-friendly and advanced tutorials as well as the technical documentation which contains detailed descriptions of all DataLad features. If you want to learn more about the underlying file system operations, the git-annex documentation is a useful resource as well."
  },
  {
    "objectID": "index.html#footnotes",
    "href": "index.html#footnotes",
    "title": "DataLad EuroScipy 2025",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nHalchenko et al., (2021). DataLad: distributed system for joint management of code, data, and their relationship. Journal of Open Source Software, 6(63), 3262, https://doi.org/10.21105/joss.03262.↩︎\nGorman, K. B., Williams, T. D., & Fraser, W. R. (2014). Ecological sexual dimorphism and environmental variability within a community of Antarctic penguins (genus Pygoscelis): https://doi.org/10.1371/journal.pone.0090081↩︎"
  },
  {
    "objectID": "about/michal.html",
    "href": "about/michal.html",
    "title": "Michał Szczepanik",
    "section": "",
    "text": "Michał graduated with BSc and MSc in Neuroinformatics, an interdisciplinary program at the Faculty of Physics, University of Warsaw. This is where he started using Python, as a first-year-student, back in 2011.\nHe completed his PhD at the Laboratory of Brain Imaging (LOBI), Nencki Institute. The PhD was part of the translational project Neural correlates of emotional contagion in humans, which applied fMRI and other methods to study observational fear conditioning. During the time, Michał became particularly interested in the data management side of neuroscience, as LOBI served as a core MRI facility.\nHis current work at the Psychoinformatics Group, INM-7, Forschungszentrum Jülich is in the area of research data management and research software development. It revolves around the data management software DataLad. His responsibilities include bug fixing, user support, technical writing, teaching – and sometmes just trying out new things.\n\nmszczepanik.eu/\n@doktorpanik@masto.ai\norcid.org/0000-0002-4028-2087"
  },
  {
    "objectID": "notebooks/exercises3.html",
    "href": "notebooks/exercises3.html",
    "title": "Part 3: Creating Backups and Sharing DataLad Datasets",
    "section": "",
    "text": "Command\nDescription\n\n\n\n\ngit init --bare ~/mydir\nCreate a --bare repository called mydir in the home directory (on Linux/macOS)\n\n\ngit init --bare \"$env:USERPROFILE\\mydir\"\nCreate a --bare repository called mydir in the home directory (on Windos)\n\n\ndatalad siblings\nList all siblings of the current dataset\n\n\ndatalad sibings add --name new --url ~/mydir\nAdd the repository at ~/mydir as a new sibling with the name new\n\n\ndatalad push --to new\nPush the dataset content to the sibling named new\n\n\n\n\nExercise 1 List all siblings of the current dataset.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\ndatalad siblings\n\n\n\n\nExercise 2 Initialize a --bare git repository at a path outside of this dataset.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nOn Linux/macOs\ngit init --bare ~/penguins_backup\nOn Windows\ngit init --bare $env:USERPROFILE\\penguins_backup\n\n\n\n\nExercise 3 add a new sibling to the dataset using the path to the newly created git repository as the --url. Then, list all siblings to confirm it was added.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\ndatalad siblings add --name backup --url ~/penguins_backup\ndatalad siblings\n\n\n\n\nExercise 4 Push the dataset to the new sibling.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\ndatalad push --to backup\n\n\n\n\nExercise 5 Move to a directory outside of this dataset and clone the new sibling dataset.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\ncd ..\ndatalad clone ~/penguins_backup"
  },
  {
    "objectID": "notebooks/exercises3.html#creating-a-backup",
    "href": "notebooks/exercises3.html#creating-a-backup",
    "title": "Part 3: Creating Backups and Sharing DataLad Datasets",
    "section": "",
    "text": "Command\nDescription\n\n\n\n\ngit init --bare ~/mydir\nCreate a --bare repository called mydir in the home directory (on Linux/macOS)\n\n\ngit init --bare \"$env:USERPROFILE\\mydir\"\nCreate a --bare repository called mydir in the home directory (on Windos)\n\n\ndatalad siblings\nList all siblings of the current dataset\n\n\ndatalad sibings add --name new --url ~/mydir\nAdd the repository at ~/mydir as a new sibling with the name new\n\n\ndatalad push --to new\nPush the dataset content to the sibling named new\n\n\n\n\nExercise 1 List all siblings of the current dataset.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\ndatalad siblings\n\n\n\n\nExercise 2 Initialize a --bare git repository at a path outside of this dataset.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nOn Linux/macOs\ngit init --bare ~/penguins_backup\nOn Windows\ngit init --bare $env:USERPROFILE\\penguins_backup\n\n\n\n\nExercise 3 add a new sibling to the dataset using the path to the newly created git repository as the --url. Then, list all siblings to confirm it was added.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\ndatalad siblings add --name backup --url ~/penguins_backup\ndatalad siblings\n\n\n\n\nExercise 4 Push the dataset to the new sibling.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\ndatalad push --to backup\n\n\n\n\nExercise 5 Move to a directory outside of this dataset and clone the new sibling dataset.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\ncd ..\ndatalad clone ~/penguins_backup"
  },
  {
    "objectID": "notebooks/exercises3.html#bonus-sharing-your-dataset-online",
    "href": "notebooks/exercises3.html#bonus-sharing-your-dataset-online",
    "title": "Part 3: Creating Backups and Sharing DataLad Datasets",
    "section": "BONUS: Sharing your Dataset online",
    "text": "BONUS: Sharing your Dataset online\n\n\n\n\n\n\n\nCommand\nDescription\n\n\n\n\nssh-keygen\nGenerate a public and private authentication key pair\n\n\ndatalad siblings\nList all siblings of the current dataset\n\n\ndatalad sibings add --name gin --url git@gin.g-node.org:/user/repo.git\nAdd the gin repository at /https://gin.g-node.org/user/repo as a new sibling with the name gin\n\n\ndatalad push --to gin\nPush the dataset content to the sibling named gin\n\n\n\n\nExercise 6 Use ssh-keygen to generate a public and private key pair (you don’t have to use a passphrase). Note the location where the public key is stored, e.g. .ssh/id_ed25519.pub. Open the .pub file and copy the whole content — it should look something like this: ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIBOYcoRKZZLWA4FWECpW2K/fTOvuRYXBnBA6gcea2bFq &lt;user&gt;@&lt;computer&gt;\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nssh-keygen\n\n\n\n\nExercise 7 Login in to your GIN account, go to your user settings and add the copied ssh key. Now datalad should be able to connect to your GIN account! \n\n\nExercise 8 Create a new repository on GIN, make sure to NOT initialize it with a README. \n\n\nExercise 9 add a new sibling to the dataset using the --url of the newly created gin repository and confirm the connection. Then, list all siblings to confirm it was added.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nFor the repository in the image above, the command would look like this:\ndatalad siblings add --name gin --url git@gin.g-node.org:/adswa/DataLad-101.git\n\n\n\n\nExercise 10 Push the dataset to the new GIN sibling. Then, open the repository in your browser to confirm the content was pushed.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\ndatalad push --to gin\n\n\n\n\nExercise 11 Move to a directory outside of this dataset and clone the new GIN sibling.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nFor the repository in the image above, the command would look like this:\ncd ..\ndatalad clone datalad clone https://gin.g-node.org/adswa/DataLad-101"
  },
  {
    "objectID": "notebooks/exercises1.html",
    "href": "notebooks/exercises1.html",
    "title": "Part 1: Working with DataLad Datasets",
    "section": "",
    "text": "In this section, you are going to clone an existing DataLad dataset and download its contents. While the datalad API is universal, the commands for navigating the data set differ between operating systems (see table below).\n\nTerminal commands\n\n\n\n\n\n\n\nLinux/macOS\nWindows\nDescription\n\n\n\n\nls -a\ndir /a\nList the content of the current directory (including hidden files)\n\n\nls -a data\ndir /a data\nList the content of the data directory\n\n\ndu -sh\ndir /s\nGet the disk usage of the current directory\n\n\ndu -sh data\ndir /s data Get the disk usage of the data directory\n\n\n\ncd data\ncd data\nChange the directory to data\n\n\n\n\nDataLad commands\n\n\n\n\n\n\nCommand\nDescription\n\n\n\n\ndatalad clone https://example.com\nClone the data set from example.com\n\n\ndatalad get folder/\nGet the file content of the folder/\n\n\ndatalad get folder/image.png\nGet the file content of the file image.png\n\n\ndatalad drop folder/\nDrop the file content of the folder/\n\n\n\n\nExercise 1 Clone the dataset from https://gin.g-node.org/obi/penguins\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\ndatalad clone https://gin.g-node.org/obi/penguins\n\n\n\n\nExercise 2 Change the directory to penguins and list the directory’s content\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nOn Linux/macOS:\ncd penguins\nls -a\nOn Windows:\ncd pegnuins\ndir /a\n\n\n\n\nExercise 3 Check the disk usage of the penguins directory\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nOn Linux/macOS:\ndu -sh\nOn Windows:\ndir /s\n\n\n\n\nExercise 4 Get the content of the examples subdirectory\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\ndatalad get examples\n\n\n\n\nExercise 5 Check the disk usage of the penguins directory again\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nOn Linux/macOS:\ndu -sh\nOn Windows:\ndir /s\n\n\n\n\nExercise 6 Drop the content of examples/chinstrap.jpg and check the disk usage again\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\ndatalad drop examples/chinstrap.jpg\nOn Linux/macOS:\ndu -sh\nOn Windows:\ndir /s"
  },
  {
    "objectID": "notebooks/exercises1.html#consuming-existing-datasets",
    "href": "notebooks/exercises1.html#consuming-existing-datasets",
    "title": "Part 1: Working with DataLad Datasets",
    "section": "",
    "text": "In this section, you are going to clone an existing DataLad dataset and download its contents. While the datalad API is universal, the commands for navigating the data set differ between operating systems (see table below).\n\nTerminal commands\n\n\n\n\n\n\n\nLinux/macOS\nWindows\nDescription\n\n\n\n\nls -a\ndir /a\nList the content of the current directory (including hidden files)\n\n\nls -a data\ndir /a data\nList the content of the data directory\n\n\ndu -sh\ndir /s\nGet the disk usage of the current directory\n\n\ndu -sh data\ndir /s data Get the disk usage of the data directory\n\n\n\ncd data\ncd data\nChange the directory to data\n\n\n\n\nDataLad commands\n\n\n\n\n\n\nCommand\nDescription\n\n\n\n\ndatalad clone https://example.com\nClone the data set from example.com\n\n\ndatalad get folder/\nGet the file content of the folder/\n\n\ndatalad get folder/image.png\nGet the file content of the file image.png\n\n\ndatalad drop folder/\nDrop the file content of the folder/\n\n\n\n\nExercise 1 Clone the dataset from https://gin.g-node.org/obi/penguins\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\ndatalad clone https://gin.g-node.org/obi/penguins\n\n\n\n\nExercise 2 Change the directory to penguins and list the directory’s content\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nOn Linux/macOS:\ncd penguins\nls -a\nOn Windows:\ncd pegnuins\ndir /a\n\n\n\n\nExercise 3 Check the disk usage of the penguins directory\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nOn Linux/macOS:\ndu -sh\nOn Windows:\ndir /s\n\n\n\n\nExercise 4 Get the content of the examples subdirectory\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\ndatalad get examples\n\n\n\n\nExercise 5 Check the disk usage of the penguins directory again\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nOn Linux/macOS:\ndu -sh\nOn Windows:\ndir /s\n\n\n\n\nExercise 6 Drop the content of examples/chinstrap.jpg and check the disk usage again\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\ndatalad drop examples/chinstrap.jpg\nOn Linux/macOS:\ndu -sh\nOn Windows:\ndir /s"
  },
  {
    "objectID": "notebooks/exercises1.html#checking-file-identity-and-location-with-git-annex",
    "href": "notebooks/exercises1.html#checking-file-identity-and-location-with-git-annex",
    "title": "Part 1: Working with DataLad Datasets",
    "section": "Checking File Identity and Location with git-annex",
    "text": "Checking File Identity and Location with git-annex\nSince DataLad is built on top of git-annex, you can use its commands on any DataLad dataset. In this section, you’ll use git-annex to get information on the dataset and localize its file contents.\n\n\n\n\n\n\n\nCommand\nDescription\n\n\n\n\ngit annex info\nShow the git-annex information for the whole dataset\n\n\ngit annex info folder/image.png\nShow the git-annex information for the file image.png\n\n\ngit annex whereis folder/image.png\nList the repositories that have the file content for image.png\n\n\n\n\nExercise 7 Display the git annex info for the file examples/gentoo.jpg. What is the size of that file? Is it present on your machine?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\ngit annex info examples/gentoo.jpg\nThe file is 4.81 megabtyes and it should be present since we previouslt loaded the content of the examples folder.\n\n\n\n\nExercise 8 Display the git-annex info of the whole data set. How many annexed files are there in the working tree?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\ngit annex info\nThe nummber of annexed files is displayed in this line: annexed files in working tree: 21\n\n\n\n\nExercise 9 Use git annex whereis to list the repositories that have the file content for the image examples/gentoo.jpg.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\ngit annex whereis examples/gentoo.jpg\n\n\n\n\nExercise 10 Use git annex whereis to list the repositories that have the file content for the table data/table_220.csv. How does this differ from the list of repositories that contain the content for gentoo.jpg?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\ngit annex whereis data/table_220.csv\nThe table is not stored in the local repository, listed in the line marked [here]."
  },
  {
    "objectID": "notebooks/exercises2.html",
    "href": "notebooks/exercises2.html",
    "title": "Part 2: Tracking Changes in DataLad Datasets",
    "section": "",
    "text": "DataLad keeps track of all changes made to your dataset. In this section, you will add new content to the penguins dataset and see how these changes are tracked in the git log of your repository.\n\n\n\n\n\n\n\nCommand\nDescription\n\n\n\n\ndatalas status\nShow any untracked changes in the current dataset\n\n\ndatalad save\nSave any untracked changes in the current dataset\n\n\ndatalad save -m \"hi\"\nSave untracked changes and add the message \"hi\"\n\n\ndatalad unlock file.txt\nUnlock file.txt to make it modifiable\n\n\ngit log\nView the dataset’s history, stored in the git log\n\n\ngit log -3\nView the last 3 entries in the git log\n\n\n\n\nExercise 1 Create a new file in the penguins folder called penguin_species.txt and add the species names gentoo and adelie. Then, save the file and run datalad status to see the untracked changes.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\necho -e \"gentoo\\adelie\" &gt; penguin_species.txt\ndatalad status\n\n\n\n\nExercise 2 Use datalad save to save the untracked changes with the message \"list penguin species\".\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\ndatalad save -m \"list penguin species\"\n\n\n\n\nExercise 3 Open the git log and find the entry created by the previous datalad save command\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nOn Linux/macOS this should be the last (i.e. top) entry, on Windows it will be the second-to-last.\ngit log\n\n\n\n\nExercise 4 Use datalad unlock to unlock the penguin_species.txt file and append chinstrap to the list. Then, run datalad save again with a message to save the changes\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\ndatalad unlock penguin_species.txt\necho -e \"chinstrap\" &gt;&gt; pegnuin_species.txt\ndatalad save -m \"add chinstrap\"\n\n\n\n\nExercise 5 Open the git log again and find the entry from the datalad save command above.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\ngit log"
  },
  {
    "objectID": "notebooks/exercises2.html#modifying-a-dataset",
    "href": "notebooks/exercises2.html#modifying-a-dataset",
    "title": "Part 2: Tracking Changes in DataLad Datasets",
    "section": "",
    "text": "DataLad keeps track of all changes made to your dataset. In this section, you will add new content to the penguins dataset and see how these changes are tracked in the git log of your repository.\n\n\n\n\n\n\n\nCommand\nDescription\n\n\n\n\ndatalas status\nShow any untracked changes in the current dataset\n\n\ndatalad save\nSave any untracked changes in the current dataset\n\n\ndatalad save -m \"hi\"\nSave untracked changes and add the message \"hi\"\n\n\ndatalad unlock file.txt\nUnlock file.txt to make it modifiable\n\n\ngit log\nView the dataset’s history, stored in the git log\n\n\ngit log -3\nView the last 3 entries in the git log\n\n\n\n\nExercise 1 Create a new file in the penguins folder called penguin_species.txt and add the species names gentoo and adelie. Then, save the file and run datalad status to see the untracked changes.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\necho -e \"gentoo\\adelie\" &gt; penguin_species.txt\ndatalad status\n\n\n\n\nExercise 2 Use datalad save to save the untracked changes with the message \"list penguin species\".\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\ndatalad save -m \"list penguin species\"\n\n\n\n\nExercise 3 Open the git log and find the entry created by the previous datalad save command\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nOn Linux/macOS this should be the last (i.e. top) entry, on Windows it will be the second-to-last.\ngit log\n\n\n\n\nExercise 4 Use datalad unlock to unlock the penguin_species.txt file and append chinstrap to the list. Then, run datalad save again with a message to save the changes\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\ndatalad unlock penguin_species.txt\necho -e \"chinstrap\" &gt;&gt; pegnuin_species.txt\ndatalad save -m \"add chinstrap\"\n\n\n\n\nExercise 5 Open the git log again and find the entry from the datalad save command above.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\ngit log"
  },
  {
    "objectID": "notebooks/exercises2.html#running-scripts-with-datalad",
    "href": "notebooks/exercises2.html#running-scripts-with-datalad",
    "title": "Part 2: Tracking Changes in DataLad Datasets",
    "section": "Running Scripts with DataLad",
    "text": "Running Scripts with DataLad\nOften, we won’t edit our dataset manually but run scripts that do so. In this section you will use DataLad to run Python scripts and track the changes made by them. You are also going to use the dataset’s history to re-run the commands.\n\n\n\n\n\n\n\nCommand\nDescription\n\n\n\n\ndatalad run \"python script.py\"\nRun the python script script.py\n\n\ndatalad run --input \"data.csv\" --output \"figure.png\" \"python script.py\"\nRun script.py with input \"data.csv\" and output \"figure.png\"\n\n\ngit log\nView the dataset’s histroy stored in the git log\n\n\ndatalad rerun a268d8ca22b6\nRerun the command from the git log with the checksum starting with a268d8ca22b6e87959\n\n\ndatalad rerun --since a268d8ca22b6\nRerun ALL commands --since the one with the checksum starting with a268d8ca22b6e87959\n\n\n\n\nExercise 6 Try to run the python script in code/aggregate_culmen_data.py. What error message do you observe?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\ndatalad run \"python code/aggregate_culmen_data.py\"\nYou should see FileNotFoundError: [Errno 2] No such file or directory because the dataset does not contain the annexed content of the required files.\n\n\n\n\nExercise 7 (Put all tables in the same folder to avoid complicated glob pattern) Run the same script with the data/ folder as --input and the file \"results/penguin_culmens.csv\" as --output.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\ndatalad run --input \"data/\" --output \"results/penguin_culmens.csv\" \"python code/aggregate_culmen_data.py\"\n\n\n\n\nExercise 8 Open the git log to view the entry created by the datalad run command. Then, copy the checksum of that commit\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\ngit log\nThe git log entry should look like this:\ncommit af78031c9ca45d3c349a692b0afd332178639e64 (master)\nAuthor: Ole Bialas &lt;ole.bialas@posteo.de&gt;\nDate:   Thu Jul 31 11:14:00 2025 +0200\n\n    [DATALAD RUNCMD] python code/aggregate_culmen_data.py\n\n    === Do not change lines below ===\n    {\n     \"chain\": [],\n     \"cmd\": \"python code/aggregate_culmen_data.py\",\n     \"dsid\": \"3a8aacc5-85f0-4114-adee-fcfa7d21a5df\",\n     \"exit\": 0,\n     \"extra_inputs\": [],\n     \"inputs\": [\n      \"data\"\n     ],\n     \"outputs\": [\n      \"results/penguin_culmens.csv\"\n     ],\n     \"pwd\": \".\"\n    }\n    ^^^ Do not change lines above ^^^\nThe checksum is displayed on the first line, after “commit”: af78031c9ca45d3c349a692b0afd332178639e64\n\n\n\n\nExercise 9 Use the copied checksum to rerun the previous datalad run command\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe command looks like this but the checksum will be different for everyone:\ndatalad rerun af78031c9ca45d3c349a692b0afd332178639e64\n\n\n\n\nExercise 10 Run the script code/plot_culmen_length_vs_depth.py — it takes results/penguin_culmens.csv as --input and produces results/culmen_length_vs_depth.png as an output.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\ndatalad run --input \"results/penguin_culmens.csv\" --output \"results/culmen_length_vs_depth.png\" \"python code/plot_culmen_length_vs_depth.py\"\n\n\n\n\nExercise 11 Use the checksum of the commit for the run command from Exercise 7 and re-run everything --since this commit (i.e. the data aggregation and plotting scripts).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe command looks like this but the checksum will be different for everyone:\ndatalad rerun --since af78031c9ca45d3c349a692b0afd332178639e64"
  },
  {
    "objectID": "slides/intro.html#a-community-project",
    "href": "slides/intro.html#a-community-project",
    "title": "Introduction to DataLad",
    "section": "A community project",
    "text": "A community project\n\n10+ years of ongoing development & maintenance 1\n100+ contributors across core, extensions, and Handbook\nstarted by:\n\nMichael Hanke (now: Psychoinformatics Lab, Forschungszentrum Jülich)\nYaroslav Halchenko (now: Center for Open Neuroscience, Dartmouth College)\n\nowes a lot to git-annex by Joey Hess & contributors\n\nhttps://doi.org/10.34734/FZJ-2025-01847"
  },
  {
    "objectID": "slides/intro.html#a-piece-of-software",
    "href": "slides/intro.html#a-piece-of-software",
    "title": "Introduction to DataLad",
    "section": "A piece of software",
    "text": "A piece of software\n\nA piece of software for data management\nWritten in Python\nBased on git and git-annex\nFOSS (MIT license)"
  },
  {
    "objectID": "slides/intro.html#exhaustive-tracking-of-research-components",
    "href": "slides/intro.html#exhaustive-tracking-of-research-components",
    "title": "Introduction to DataLad",
    "section": "Exhaustive tracking of research components",
    "text": "Exhaustive tracking of research components\n Well-structured datasets (using community standards), and portable computational environments — and their evolution — are the precondition for reproducibility\n\n\n# turn any directory into a dataset\n# with version control\n\n% datalad create &lt;directory&gt;\n\n# save a new state of a dataset with\n# file content of any size\n\n% datalad save"
  },
  {
    "objectID": "slides/intro.html#capture-computational-provenance",
    "href": "slides/intro.html#capture-computational-provenance",
    "title": "Introduction to DataLad",
    "section": "Capture computational provenance",
    "text": "Capture computational provenance\n Which data were needed at which version, as input into which code, running with what parameterization in which computional environment, to generate an outcome?\n\n\n# execute any command and capture its output\n# while recording all input versions too\n\n% datalad run --input ... --output ... &lt;command&gt;"
  },
  {
    "objectID": "slides/intro.html#exhaustive-capture-enables-portability",
    "href": "slides/intro.html#exhaustive-capture-enables-portability",
    "title": "Introduction to DataLad",
    "section": "Exhaustive capture enables portability",
    "text": "Exhaustive capture enables portability\n Precise identification of data and computational environments, combined for provenance records form a comprehensive and portable data structure, capturing all aspects of an investigation.\n\n\n# transfer data and metadata to other sites and services\n# with fine-grained access control for dataset components\n\n% datalad push --to &lt;site-or-service&gt;"
  },
  {
    "objectID": "slides/intro.html#reproducibility-strengthens-trust",
    "href": "slides/intro.html#reproducibility-strengthens-trust",
    "title": "Introduction to DataLad",
    "section": "Reproducibility strengthens trust",
    "text": "Reproducibility strengthens trust\n Outcomes of computational transformations can be validated by authorized 3rd-parties. This enables audits, promotes accountability, and streamlines automated “upgrades” of outputs.\n\n\n# obtain dataset (initially only identity,\n# availability, and provenance metadata)\n\n% datalad clone &lt;url&gt;\n\n# immediately actionable provenance records\n# full abstraction of input data retrieval\n\n% datalad rerun &lt;commit|tag|range&gt;"
  },
  {
    "objectID": "slides/intro.html#ultimate-goal-reusability",
    "href": "slides/intro.html#ultimate-goal-reusability",
    "title": "Introduction to DataLad",
    "section": "Ultimate goal: (re)usability",
    "text": "Ultimate goal: (re)usability\n\nVerifiable, portable, self-contained data structures that track all aspects of an investigation exhaustively can be (re)used as modular components in larger contexts — propagating their traits\n# declare a dependency on another dataset and\n# reuse it at particular state in a new context\n\n% datalad clone -d &lt;superdataset&gt; &lt;path-in-dataset&gt;"
  },
  {
    "objectID": "slides/intro.html#a-meta-view",
    "href": "slides/intro.html#a-meta-view",
    "title": "Introduction to DataLad",
    "section": "A meta view",
    "text": "A meta view"
  },
  {
    "objectID": "slides/intro.html#provider-of-identifiers",
    "href": "slides/intro.html#provider-of-identifiers",
    "title": "Introduction to DataLad",
    "section": "Provider of identifiers",
    "text": "Provider of identifiers\n\nDataLad provides globally unique, persistent identifiers (without a central issuing service; offline and portable)\nConcept identifiers\n\nfor datasets: DataLad dataset ID\nfor files in a dataset: DataLad dataset ID + path within a dataset\n\nContent/version identifiers\n\nfor datasets: Git commit SHA ID\nfor files: Git blob SHA / Git-annex key"
  },
  {
    "objectID": "slides/intro.html#what-is-a-datalad-dataset",
    "href": "slides/intro.html#what-is-a-datalad-dataset",
    "title": "Introduction to DataLad",
    "section": "What is a DataLad dataset?",
    "text": "What is a DataLad dataset?\n\nmetadata on the evolution of a collection of files\n\ncontent identity  think: checksums\ncontent availability  think: URLs\nprovenance of change  think: who did what when?\n\na regular, but managed directory on computer file system think: provides the familiar look and feel of files and folders\na Git repository  think: compatible with anything that can handle Git repositories\nemploys git-annex for file content tracking and transport think: supports any storage service and transport protocol supported by git-annex\nextends both Git and git-annex with additional features and infrastructure support  think: deposit and retrieve from non-Git-aware sites"
  },
  {
    "objectID": "about/ole.html",
    "href": "about/ole.html",
    "title": "Ole Bialas",
    "section": "",
    "text": "Ole …"
  }
]